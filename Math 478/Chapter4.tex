\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{dsfont}

\usepackage{graphicx}

\newcommand{\prob}{\mathds{P}}
\newcommand{\expec}{\mathds{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\ex}[1]{\textbf{Example #1}}


\setlength\parindent{0pt}

\author{Pranav Tikkawar}
\title{Chapter 4}

\begin{document}
\maketitle

\textbf{9/3}
\section*{Chapter 4}
\subsection*{Markov Property}
If the probability of the nest state only depends on the current state, it satisfies the "Markov Property".\\
\textbf{Drunkards walk example}
$$\mathds{P}(x_{i+1} = x_i \pm 1) = \frac{1}{2} \mathds{P}(x_{i+1} \neq x_i \pm 1) = 0$$
$$ \mathds{P}(x_{i+1} = x+1 | x_i = x) = 1/2$$
$$ \mathds{P}(x_{i+1} = x-1 | x_i = x) = 1/2$$
\subsection*{Formal Definition}
Let $\{X_n, n \in \mathds{N}\}$ be a stochastic process that takes discrete time values. Suppose $\mathds{P}(X_{n+1} = j | X_n = i_n ... X_0 = i_0) = P_{i,j}$ Such a stochastic process is called a Markov Chain. $P_{ij}$ is the transition probability from state i to state j.
\subsection*{Transition Probability Matrix}
Let $i,j \in \mathds{N}$ be possible states of the Markov Chain. The matrix $P = [P_{ij}]$ is called the transition probability matrix of the Markov Chain. Where $P_{ij} = \mathds{P}(x_{n+1} = j | x_n = i)$.
\textbf{Ex 4.1}
$$\mathds{P}(\text{rain tomorrow} | \text{rain today}) = \alpha$$
$$\mathds{P}(\text{rain tomorrow} | \text{no rain today}) = \beta$$
$$\text{Let} \begin{cases}
    0 = \text{rain} \\
    1 = \text{no rain}
\end{cases}$$
$$P = \begin{bmatrix}
    \alpha & 1 - \alpha \\
    \beta & 1 - \beta
\end{bmatrix}$$
\textbf{Ex 4.4}
Suppose whether it rains tomorrow or not depends on both todays and yesterdays weather. \\
$$\begin{tabular}{|c|c|c|}
    \hline
    Today's Weather & Yesterdays's Weather & Value \\
    \hline
    Rain & Rain & 0 \\
    \hline
    Rain & No Rain & 1 \\
    \hline
    No Rain & Rain & 2 \\
    \hline
    No Rain & No Rain & 3 \\
    \hline
\end{tabular}$$
Suppose:\\ $\mathds{P}(\text{rain tomorrow} | \text{rain today, rain yesterday}) = .7$\\
$\mathds{P}(\text{rain tomorrow} | \text{rain today, no rain yesterday}) = .5$\\
$\mathds{P}(\text{rain tomorrow} | \text{no rain today, rain yesterday}) = .4$\\
$\mathds{P}(\text{rain tomorrow} | \text{no rain today, no rain yesterday}) = .2$\\
$$P = \begin{bmatrix}
    .7 & 0 & .3 & 0 \\
    .5 & 0 & .5 & 0 \\
    0 & .4 & 0 & .6 \\
    0 & .2 & 0 & .8
\end{bmatrix}$$
\section*{4.2 Chapman-Kolmogorov Theorem}
$P_{ij}$ = probability of going from state i to state j\\
$P_{ij}^{(n)}$ = probability of going from state i to state j in n steps.\\
$P_{ij}^{(n+m)} = \sum_{k} P_{ik}^{(n)} P_{kj}^{(m)}$ (pg.197)\\
\textbf{Look at example 4.10 for next class}

\textbf{9/8}
\subsection*{Proof of Chapman-Kolmogorov Theorem}
Equation: $P_{ij}^{(n+m)} = \sum_{k} P_{ik}^{(n)} P_{kj}^{(m)}$\\
We can visualize this as a graph with n+m steps and we consider all the paths $i \rightarrow j$ and sum them with the law of total probability.\\
\textbf{Proof:}\\
$$ P_{ij}^{(n+m)} = \mathds{P}(X_{n+m} = j | X_0 = i)$$
$$ = \sum_{k} \mathds{P}(X_{n+m} = j, X_n = k | X_0 = i)$$
$$ = \sum_{k} \mathds{P}(X_{n+m} = j | X_n = k, X_0 = i) \mathds{P}(X_n = k | X_0 = i)$$
Note that this is the probabilty of going from k to j in m steps(which doesn't depend on $x_0 =i$ due to the Markov Property) and from i to k in n steps.\\
Homogeneity of a Markov Chain.\\
\textbf{Example 4.10} 
An urn always contains 2 balls. Possible ball colors are red and blue. Each stage of the process we pick a ball and randomly replace it with another ball. Replacement of the same color is $.8$ and replacement of a different color is $.2$.\\ 
If initially both the first balls are red, what is the probability that the 5th ball is red?\\   
$$P = \begin{bmatrix}
    .8 & .2 & 0 \\
    .1 & .8 & .1 \\
    0 & .2 & .8
\end{bmatrix}$$
Note: for a set up where the probability of changing colors is invariant of the color of the ball, the transition matrix will be visually "radially" symmetric***.\\
$$\mathds{P}(X_5 = \text{red} ) = P_{22}^{(4)}  + \frac{1}{2}P_{21}^{(4)} + 0P_{12}^{(4)}$$
$$ =0.7048 $$
Ask what are other Properties of stochastic matrix\\
$ a_{i,j} = a{n-i, n-j }$\\
\ex{4.11}\\
In a sequence of independent flips of a fair coint, let $N$ denote the number of flips until there is a run of 3 heads.\\
Find (a) $P(N \leq 8)$ (b) $P(N = 8)$\\
Consider 4 states: 0,1,2,3. given by n = the number of consecutive heads\\
$$P = \begin{bmatrix}
    1/2 & 1/2 & 0 & 0 \\
    1/2 & 0 & 1/2 & 0 \\
    1/2 & 0 & 0 & 1/2 \\
    0 & 0 & 0 & 1 \\
\end{bmatrix}$$
(a) = $P_{03}^{(8)}$\\
(b) = $\frac{1}{2} P_{02}^{(7)}$\\
\subsection*{4.3 Classification of States}
\textbf{Definition:} State j of is accessible from state i if $P_{ij}^{(n)} > 0$ for some $n \geq 0$. If the states are accessible from each other, they are said to communicate.\\
Communication is an equivalence relation.\\
Reflexive and symmetric are obvious.\\
Transitive is proven by the Chapman-Kolmogorov Theorem.\\
This relation divides the states into classes.\\
\subsection*{Reccurent and Transient States}
\textbf{Definition:} A given state i of a Markov Chain let $f_i$ denote the probability that the chain will eventually return to state i.\\
A state is called \textbf{Recurrent} if $f_i = 1$ and \textbf{Transient} if $f_i < 1$.\\
The expected number of revisits to a recurrent state is infinite.\\
For a transient state the probability of being in state i for exactly n times period is $f_i^n (1-f_i)$: Note that this is Geometric distribution\\
\textbf{Lets notice state properties:}\\
\begin{align*}
    f_i &= \prob(x_{n+N} =i | X_n = i)\\
    & = \prob(x_N = i | X_0 = i)
\end{align*}

A Recurrent state is revisited infinitely often after it is visited once it will be revisited by the markov properties, and it repeats.\\
A Transient state is revisited only a finite number of times.\\
\textbf{Proof of Transitive state finite recurrence:}\\
The probability a transient state is revisited exactly n times is $f_i^{n-1} (1-f_i)$\\
\begin{align*}
    E(n) &= \sum_{n=1}^{\infty} n f_i^{n-1} (1-f_i)\\
    &= (1-f_i)\sum_{n=1}^{\infty} \frac{d}{df_i} f_i^n\\
    &= (1-f_i)\frac{d}{df_i} \sum_{n=1}^{\infty} f_i^n\\
    &= (1-f_i)\frac{d}{df_i} \frac{f_i}{1-f_i}\\
    &= \frac{1}{(1-f_i)}
\end{align*}
\textbf{Proposition 4.1}\\
A state is is 
\begin{enumerate}
    \item Recurrent if $\sum_{n=1}^{\infty} P_{ii}^{(n)} = \infty$\
    \item Transient if $\sum_{n=1}^{\infty} P_{ii}^{(n)} < \infty$
\end{enumerate}
\textbf{Proof:}\\
Define $I_n = \begin{cases}
    1 & \text{if } X_n = i\\
    0 & \text{otherwise}
\end{cases}
$\\
The number of times period the process is in state i is $\sum_{n=0}^{\infty} I_n$\\
The expected value of the number of times the process is in state i is
\begin{align*}
    \expec(\sum_{n=0}^{\infty} I_n) &= \sum_{n=0}^{\infty} \expec(I_n)\\
    &= \sum_{n=0}^{\infty} \prob(x_n = i |x_0 = i)\\
    &= \sum_{n=0}^{\infty} P_{ii}^{(n)}
\end{align*}
\textbf{Corallary 4.2 (pg 207)}\\
If i is recurrent and i communicates with j, then j is recurrent.\\
\textbf{Proof:}\\
$$i \leftrightarrow j \rightarrow \exists k \text{ s.t. } P_{ij}^k > 0 \text{ and } P_{ji}^k$$ For any n, $$P_{ij}^{(m+n+k)} \geq P_{ji}^m P_{ii}^n P_{ij}^k$$
$$ \sum_{n=1}^{\infty}P_{ij}^{(m+n+k)} \geq \sum_{n=1}^{\infty} P_{ji}^m P_{ii}^n P_{ij}^k$$
$$\sum_{t=0}^{\infty} P_{jj}^t \geq \sum_{n=1}^{\infty}P_{ij}^{(m+n+k)} \geq P_{ji}^m P_{ij}^k sum_{n=1}^{\infty}  P_{ii}^n \geq \infty $$
Thus if i is recurrent and i communicates with j, then j is recurrent.\\
\textbf{Remark:} If the state i is transient and if the state j communicates with i, then j is transient.\\
\textbf{Proof:} 
Assume the if, Suppose j is not transient. Then j is recurrent. Then i is recurrent. This is a contradiction. Thus j is transient.\\
\textbf{Remark:} Transience and Recurrence are class properties.\\ 
\textbf{Remark:} Suppose we have a Markov Chain with a finite number of states. Then at least one state is recurrent.\\
A Markov Chain with exactly one communication class is called irreducible.\\
A finite state irreducible Markov Chain must have all states recurrent.\\
\textbf{Example 4.18}
Consider a Markov Chain with 5 states.\\
$$P = \begin{bmatrix}
    .5 & .5 & 0 & 0 & 0 \\
    .5 & .5 & 0 & 0 & 0 \\
    0 & 0 & .5 & .5 & 0 \\
    0 & 0 & .5 & .5 & 0 \\
    .25 & .25 & 0 & 0 & .5
\end{bmatrix}$$
Find the equivalence classes, classify them as reccurent or transient.\\
\textbf{Solution:}\\
The equivalence classes are $\{0,1\}$ and $\{2,3\}$ and $\{4\}$\\
4 is its own class due to the fact the communication between 0 and 1 is not symmetric.\\
Does number of nonzero eigenvectors equal the number of equivalence classes?\\
\textbf{Example 4.19}\\
Markov Chain wih states $(0,\pm 1 \dots)$\\
$P_{i,i+1} = p$ and $P_{i,i-1} = 1-p$\\
pg 209-211 \\
A random walk is symmetric if $p = 1/2$\\
Can prove that the random walk recurrent in that case
\textbf{Remark:} \\
Definition of Reccurence:
$$ f_i = P(\text{Ever coming back to state i | starting at state i})$$
$$ f_i = P(\sum_n (x_n = i) | x_0 = i)$$
\textbf{Random Walk}\\
State space is $\mathscr{Z}$\\
TRansition probabilities are: $P_{i,i+1} = p$ and $P_{i,i-1} = 1-p $\\
Note: when there is only one equivalence class, the Markov Chain is irreducible.\\
Find $f_0 = \beta = P(\text{ever returning to 0 | starting at 0})$\\
Condition the probability $\beta$ on the next transtion.
$$\beta =  (p)P(\text{ever returns to 0} | x_1 = 1) + (1-p)P(\text{returns to 0} | x_1 = -1)$$
Let $\alpha = P(\text{ever returns to 0} | x_1 = 1)$\\
$ \alpha = (p)P(\text{ever returns to 0} | x_1 = 1, x_2 = 2) + (1-p)P(\text{ever returns to 0} | x_1 = 1, x_2 = 0)$\\
$$\alpha = 1 - p + p\alpha^2$$
Solving gives $$\alpha = \frac{1-p}{p}$$
If the random walk is symetric then $\alpha = 1$ is the only Solution. 
Substitution in equation with beta gives $f_0 = 1$\\
\section*{4.4 Long Run Proportions and Limiting Probability}
Let $i \neq j$ be states of a Markov Chain.\\
Define $f_{ij}$ as the probability that the Markov chain, starting in state i, will ever reach state j.\\
$$f_{ij} = \sum_{n=1}^{\infty} P_{ij}^{(n)}$$
\textbf{Proposition 4.3}\\
If the state i is reccurent and i communicates with j, then probability of eventually reaching j is 1, $f_{ij} = 1$.\\
\textbf{Proof:}\\
$$i \leftrightarrow j \rightarrow \exists n > 0 \text{ s.t. } P_{ij}^{(n)} > 0$$ Assume n is the minimum such integer.\\
Since i is recurrent, the infinite sequence $0 = k_0 < k_1 < k_2 < ...$ exists such that $X_{k_r} = i$ for $r = 0,1,2,3,...$\\
Define $z = min( r> 0 , X_{k_r + n  } = j)$\\
Then $P(Z = z) =  P^n_{ij} (1 - P_{ij}^{(n)})^{z-1}$\\ 
Thus $f_{ij} = 1$ by sandwich Theorem\\
Assume j is a recurrent state.\\
Define $N_j = min(n > 0 | X_n = j)$\\
Let $m_j = E[N_j | x_0 = j]$\\
It is the expextected number of steps to return to j.\\
Since we know that $P(N_j < \infty| x_0 = j) = 1$\\
Still it may happen that $m_j = E[N_j | x_0 = j] = \infty$\\
Definition: if $m_j < \infty$ then j is positive recurrent.\\
If $m_j = \infty$ then j is null recurrent.\\
We define $\pi_j$ to be the long run proportion of time the Markoc chain is in state j.\\
$$\pi_j = \lim_{n \to \infty} \frac{1}{n} \sum_{k=0}^{n-1} I_k$$
Where $I_k = 1$ if $X_k = j$ and $0$ otherwise.\\
Proposition 4.4 :\\
IF the markov chain is irreducible and reccurent then any intial state $x_0$ will have $\pi_j = \frac{1}{m_j}$\\
At time $T_0 + \sum T_k$ the chain enters state j for the (n+1)th time, the proporttion of th time the chain is in state j during this is $\frac{n+1}{T_0 + \sum T_k}$\\
$$\pi_j = \lim_{n \rightarrow \infty} \frac{n+1 }{T_0 + \sum T_k }$$
Prop 4.4 if a MC is irreducible and recurrent, then $\pi_j = \frac{1}{m_j}$\\
Propr 4.5 if the state i is positive reccurent and if the state j communicates with i then the state j is also positive recurrent.\\
Proof: $i \Leftrightarrow j \rightarrow \exists n>0 \text{ such that } P^n_{ij} > 0, \pi_i P^{n}_{ij} =$ Proportion of times that the pricess wil be in state j, n steps after it was in step i. $ < \pi_j$    \\
$$\pi_i P^n_{ij} \leq \pi_j$$
$pi_i > 0 $ since it is positive reccurent. since P is also finite thus $\pi_j$ is positive reccurent.\\
Remark: null recurrence is also a class property.\\
\textbf{Claim:} an irreducible finite state markov chain is positive recurrent.\\
\textbf{Proof:} Let $m_j$ be the expected return time to state j.\\
If you have a finite MC then the there is one EC, and if one is null recurrent then all are null recurrent.\\
Suppose that satte i in such  MC is null recurrent.\\
Then $\pi_j = 0$ Since null reccurnece is a class property and there is only one class, thus all states are null recurrent.\\
$\pi_i = 0$ for all states.\\
$\sum pi_i = 0$ with probability one. This is a contradiction.\\
\textbf{Theorum 4.1} Consider a irreducible Markov Chain. If the chain is positive reccurent then the long run proportions are unique colutions of the system of equations 
$$\sum_{i} \pi_i P_{ij} = \pi_j$$
and $\sum_{j} \pi_j = 1$\\
Think of it like all way the ways the $\pi$ go to state j. \\
Similar to conservation of flow.\\
\textbf{Matrix Intutition}:\\
Write $\vec{\pi} = \begin{bmatrix}
    \pi_0 & \pi_1 & \pi_2 & \dots
\end{bmatrix}$\\
be the row vector with entris $\pi_j$\\
Then $\vec{\pi} P = \vec{\pi}$\\
And $\sum_j \pi_j = 1$\\
\textbf{Example 1}\\
Consider a two state Markov Chain with transition matrix 
$$ P = \begin{bmatrix}
    \alpha & 1 - \alpha \\
    \beta & 1-\beta
\end{bmatrix}$$
Compute the long run proptions $\pi_0$ and $\pi_1$\\
Assume $\alpha , \beta \neq 0,1$\\
$$\pi_0 P_{01} + \pi_1 P_{11} = \pi_1$$
$$\pi_0 P_{00} + \pi_1 P_{10} = \pi_0$$
$$\pi_0 + \pi_1 = 1$$
In matrix formulation we have $\begin{bmatrix}
    \pi_0 & \pi_1
\end{bmatrix} \begin{bmatrix}
    \alpha & 1 - \alpha \\
    \beta & 1-\beta
\end{bmatrix} = \begin{bmatrix}
    \pi_0 & \pi_1
\end{bmatrix}$\\
Short cut to remember 
$$ \begin{bmatrix}
    & 0 & 1 \\
    0 & P_{00} & P_{01}\\
    1 & P_{10} & P_{11}\\
    & \pi_0 & \pi_1 \\
\end{bmatrix}
$$
Note that $\pi_0 = \frac{\beta}{1- \alpha + \beta}$ and $\pi_1 = \frac{1-\alpha}{1 - \alpha + \beta}$\\
\textbf{Example 2}\\
Doubly stochastic matrix. If the sum of the columns and rows are equal to 1.\\
If the transition matri of a Markov Chain with n-states is a doubly stochastic matrix , then the long run proportions are $\pi_j = 1/n$ for all j.\\
\textbf{Proof:}
$$[1/n, 1/n, 1/n, \dots] P = [1/n, 1/n, 1/n, \dots]$$
$$ 1/n \sum_{j} P_{ij} = 1/n$$
$$\sum_{j} P_{ij} = 1$$
Since solutions are unique as in Theroum 4.1, thus $\pi_j = 1/n$ for all j.\\
\textbf{Example 3}\\
Simple random symmetric walk. This is also reflectant at the edges.\\
States are $(0, 1, 2, \dots, L)$\\
$P_{01} = 1, P_{L,L-1} = 1$\\
$P_{i,i-1} = 1/2$ and $P_{i,i+1} = 1/2$\\
Simple case: L = 2\\
$$ P = \begin{bmatrix}
    0 & 1 & 0 \\
    1/2 & 0 & 1/2 \\
    0 & 1 & 0
\end{bmatrix}
$$
$$\pi_0 = 0, \pi_1 = 1/2, \pi_2 = 1/2$$
Case L = 3\\
$$ P = \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1/2 & 0 & 1/2 & 0 \\
    0 & 1/2 & 0 & 1/2 \\
    0 & 0 & 1 & 0
\end{bmatrix}$$
$$\pi_0 = 1/6 , \pi_1 = 1/3 , \pi_2 = 1/3 , \pi_3 = 1/6$$
Prove that for L = n, $\pi_i = \frac{1}{n}$ for all i other than $\pi_0, \pi_L = 1/2L$.\\
\textbf{Example}\\
For L = 1000, what is the probability of revisiting state 0?\\
it is 2000 as it is $\pi = 1/m$\\
If the system is inconsistent the MC is transient or null reccurent.\\
$\pi_j = 0 $ for all j.\\
\textbf{Example 4.26:}\\
MC with acceptable staus in the set $A$ and unacceptable status $A^C$\\
If $x_n \in A$ process is "up", if $x_n \in A^C$ process is "down".\\
Find: \\
\textbf{i:} Rate at which the process goes from up to down.\\
\textbf{ii:} Average length of time process remain down when it goes down.\\
\textbf{iii:} Average length of time process remains up when it goes up.\\
\textbf{Solution:}\\
Let $i \in A$ and $j \in A^C$\\
The rate at which the process enters j from i is $$= \pi_i P_{ij}$$\\
Rare ar which process enters j from any acceptable state is $$\sum_{i \in A} \pi_i P_{ij}$$
Rate at which the process goes from $A \rightarrow A^C$ is $$\sum_{j \in A^C} \sum_{i \in A} \pi_i P_{ij}$$
Let $u$ be the average time process stays up.\\
Let $d$ be the average time process stays down.\\
rate at which a breakdown occurs is $\frac{1}{u+d}$\\
Therefore $$\frac{1}{u+d} = \sum_{j \in A^C} \sum_{i \in A} \pi_i P_{ij}$$
Proportion the process is up is 
$$\frac{u}{u+d} = \sum_{i \in A} \pi_i$$
Get $u$ and $d$ in terms of $\pi$\\
$$u = \frac{\sum_{i \in A} \pi_i}{\sum_{j \in A^C} \sum_{i \in A} \pi_i P_{ij}}$$
$$d = \frac{\sum_{j \in A^C} \sum_{i \in A} \pi_i P_{ij}}{\sum_{i \in A^C} \pi_i}$$

\textbf{Stationary Probability}\\
If the intial distribution of states is chosen accoring to long run propotions $\pi_j$, then the future distrubituion of the state of the system will be the same if $P(x_0 = j) = \pi_j$ then $P(x_n = j) = \pi_j$\\
Using induction we can see this true for all n. (4.4)\\

\textbf{4.4.1}:
Limiting probabilities: 
\textbf{Example: } Consider a two state MC with $$P = \begin{bmatrix}
    .7 & .3 \\
    .4 & .6
\end{bmatrix}$$
Numericcal calculation show that $P^{(n)}$ converges to a limiting distribution $$P^n = \begin{bmatrix}
    4/7 & 3/7 \\
    4/7 & 3/7
\end{bmatrix}
$$
\textbf{Claim: } The limiting probabilties $\lim_{n \rightarrow \infty} P(x_n = j) $ if they exist are equal to the long run proportions $\pi_j$\\
\textbf{Proof:} Assume $\alpha_j = \lim_{n \to \infty} P(x_n = j)$ exists.\\
Then $P(x_{n+1} = j) = \sum_{i} P(x_{n+1} = j | x_n = i) P(x_n = i)$\\
Gives $\lim_{n \to \infty} P(x_{n+1} = j) = \lim_{n \to \infty} \sum_{i} P_{ij} P(x_n = i)$\\
Thus $\alpha_j = \sum_{i} P_{ij} \alpha_i$ for all $j$\\
Also $\sum_{j} \alpha_j = 1$\\
Recall that $pi_j$ are the unique solutions of the system of equations $$\sum_{i} \pi_i P_{ij}, \sum_{i}\pi_j = 1$$
Therefore $\lim_{n \rightarrow \infty }\alpha_j = \pi_j$ if $\alpha_j$ exists \\
When do limits not exist? When $n \rightarrow \infty$ diverges or collates \\
A chain that can only return to a state a multiple of $d>1$ times is called a periodic chain. And does not have limiting probabilties.\\
\textbf{Definition: }\\
An irrecucible, positive reccurent, aperiodic Markov Chain is said to be ergodic.\\
\textbf{Branching Proess:}\\
A branching process is a Markov Chain with time give vy generations in $0,1,2,3, \dots$ and states fiven by populations in $0,1,2,3, \dots$\\
Induviduals in each gen eration produce offspring
$$X_i = \# of offspring of individual of the (i-1)^th generations$$
\textbf{Remark}\\
If 0 is a reccurent state because $P_{00} = 1$\\
Then it is an abosrbing state.\\
Proof is somewhat trivial using matrix multiplication.\\
\textbf{Remark 2}\\
Define $P_0 = P[\text{An individual produces 0 offspring}]$ \\
If $P_0 >0$ then all the states other than 0 are transient.\\
\textbf{Proof:}\\
Consider $P_{i0}$\\
it is th probability for going from state i to 0.\\
$$= P[Each one of the i Individuals produces 0 offspring]$$
$$= P_0^i$$
$$= P_0^i > 0$$
Thus the state i is transient for $i \neq 0$\\
\textbf{Remark 3}
If $P_0 > 0 $ then the population eventually either becomes extinct or grows indefinitely.\\
\textbf{Note:}\\
We do not use transient probabilities to study branching processes. We mostly use the probability distribution of the number of offspring produced by an individuals. \\
Let $P_j = P[\text{An individual produces j offspring}]$\\
Compute the mean and variance of $X_n$\\
\textbf{Mean}\\
$X_0 = 1$
$$\expec(X_n) = \mu = \sum_{j} j P_j$$
$$\expec(X_n) = \expec[\expec[X_n|X_{n-1}]]$$
write $z_i, i = 1,2,3, \dots$ for the number of offspring of the $x_{n-1}$ individuals in the $(n-1)^th$ generation.\\
Then $X_n = \sum_{i=1}^{X_{n-1}} Z_i$\\
$$\expec(X_n) = \expec[\expec[\sum_{i=1}^{X_{n-1}} Z_i| X_{n-1}]]$$
$$ \mu = z_i | X_{n-1} $$
$$ E[X_n]= \expec( \mu X_{n-1})$$
$$ E[X_n] = \mu \expec(X_{n-1})$$
Since $X_0 = 1$\\
Thus $\expec[X_n] = \mu^n$\\
\textbf{Variance}
$$\var(X_n) = \sigma^2 = \sum_{j} j^2 P_j - \expec(X_n)^2$$
We can also note that 
$$ var(X_n) = \sigma^2(\mu^{n-1} + \mu^{n} + .. + \mu^{2n-2})$$
$$ var(X_n) = \begin{cases}
    \sigma^2 \mu^{n-1} \frac{1 - \mu^{n-1}}{1 - \mu} & \text{if } \mu \neq 1\\
    n \sigma^2  & \text{if } \mu = 1 
\end{cases}
$$
Probabilityt of extinction (of a population) \\
$$\pi_0 = P[\text{Population becomes extinct}]$$
$$\pi_0 = \lim_{n \to \infty} P(X_n = 0|X_0 = 1)$$

Case 1: if $\mu < 1$ then $\pi_0 = 1$\\
\textbf{Proof:}\\
We can see this that for each generation the population decreases.\\
Thus the long run proportion of the population being 0 is 1.\\
Case 2: if $\mu > 1$ then $\pi_0 < 1$\\
\textbf{Proof:}\\
This follows since the population grows indefinitely.\\
The equation for $\pi_0$ is
$$\pi_0 = \textbf{Population dies out}$$
$$= \sum_{j=0}^{\infty} P(\text{Population dies out} | x_1 = j)P_j$$
$$ = \sum_{j=0}^{\infty} \pi_0^j P_j$$
For $\mu > 1$ it can be shown that $\pi_0$ is the smallest solution to the equation $\pi_0 = \sum_{j=0}^{\infty} \pi_0^j P_j$\\

\textbf{Example 4.34}\\
Suppose $P_0 = 1/2, P_1 = 1/4, P_2 = 1/4$\\
Compute $\pi_0$.\\
Find $\mu = 0(1/2) + 1(1/4) + 2(1/4) = .75$\\
Since $\mu < 1$ then $\pi_0 = 1$\\

\textbf{Example 4.35}\\
Suppose $P_0 = 1/4 , P_1 = 1/4 , P_2 = 1/2$\\

\section*{4.8 Time Reversible Markov Chains}
\textbf{Detour:}\\
Better understand the concept of a stationary M.C.\\
It has a stationary distribution $\vec{\pi} = [\pi_1, \dots, \pi_n]$ that satisfies $\vec{\pi} P = \vec{\pi}$\\ 
Comnsider a time series that is states at a time, as it propogrates, it reaches a stationary distribution. This implies that the distribution of the states at time $n$ is the same as the distribution at time $n+1$\\
\textbf{Example:}
MC with TPM:
$$P = \begin{bmatrix}
    1/3 & 2/3\\
    1/2 & 1/2
\end{bmatrix}$$
We can calc the stationary distribution by solving the system of equations:
$$\pi_1 = \frac{1}{3}\pi_1 + \frac{1}{2}\pi_2$$
$$\pi_2 = \frac{2}{3}\pi_1 + \frac{1}{2}\pi_2$$
$$\pi_1 + \pi_2 = 1$$
This gives $\pi_1 = 3/7$ and $\pi_2 = 4/7$\\
Suppose at time $t=0$ the probability distribution is:
$$ P(x_0 = 0) = 3/7 \text{ and } P(x_0 = 1) = 4/7$$
Then at time $t=1$ the distribution is the same as at time $t=0$\\

\textbf{Time Reversible Markov Chains}\\
Consider an ergodic MC that has been running for a long time.\\
Consider the revise process $X_n, X_{n-1}, \dots, X_0$ starting for some large n.\\ 
It satisfies the markov property, future given the present is independent of the past.\\
The Transition probabilties of the rversed chain are given by $Q_{ij} = P(X_{n-1} = j | X_n = i) = P(X_m = j | X_{m+1} = i)$\\
We can find it out using bayes formula\\
$$Q_{ij} = \frac{P(X_m =j) P(X_{m+1} = i | X_m = j)}{P(X_{m+1} = i)}$$
$$Q_{ij} = \frac{\pi_j P_{ji}}{\pi_i}$$
\textbf{Definition:} A Markov Chain is time reversible if $Q_{ij} = P_{ij}$\\
If the MC is time reversible 
$$ P_{ij} = \frac{\pi_j P_{ji}}{\pi_i}$$
$$ \pi_i P_{ij} = \pi_j P_{ji}$$
This is saying the rate you go from i to j is the same as j to i\\
Verify reversible by computing $\vec{\pi} = $ and checking if $Q_{ij} = P_{ij}$\\
But more efficielty we can fin $x_i > 0$ such that $\sum x_i = 1$ and $x_iP_{ij} = x_j P_{ji}$\\
\textbf{Proof:}\\
$$\sum_i x_i P_{ij} = \sum_i x_j P_{ji}$$
$$\sum_i x_i P_{ij} = x_j \sum_i P_{ji}$$
$$\sum_i x_i P_{ij} = x_j$$
and $\sum x_i = 1$\\
By theorem $4.1$ the solutions fo this sysmen are unique and are $\vec{\pi}$\\
\textbf{Example 4.38}\\
Consider an aribitrary connected graph accociate with a (+ve) number $w_{ij}$ for each edge. Conside a particle moving from node to node such that the particle will move from node i to node j with probability $P_{ij} = \frac{w_{ij}}{d_i}$ where $d_i = \sum_j w_{ij}$\\
The TPM is:
$$P = \begin{bmatrix}
    w_{11}/d_1 & w_{12}/d_1 & \dots \\
    w_{21}/d_2 & w_{22}/d_2 & \dots \\
    \vdots & \vdots & \vdots
\end{bmatrix}$$
For this example the TPM is:\\
$$P = \begin{bmatrix}
    0 & 1/2 & 0 & 1/6 & 1/3 \\
    1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 \\ 
    1/5 & 0 & 0 & 0 & 4/5 \\
    1/6 & 0 & 1/6 & 2/3 & 0
\end{bmatrix}$$
Time reversibility for such an MC is given by:
$$\pi_i P_{ij} = \pi_j P_{ji}$$
$$\pi_i \frac{w_{ij}}{d_i} = \pi_j \frac{w_{ji}}{d_j}$$
$$\frac{\pi_i}{d_i} = \frac{\pi_j}{d_j} = c$$
$$ \pi_i = c d_i$$
$$ c = \frac{1}{\sum_i \sum_j w_{ij}}$$
Thus 
$$\pi_i = \frac{\sum_{j}w_{ij}}{\sum_i \sum_j w_{ij}}$$
Note you need to calc twice for ij and ji\\
Note if we pick all the $w_{ij} $ to be the same we get a random walk on a graph\\
Consider the MC with TPM:
2/3 going clockwise and 1/3 going counter clockwise with 3 states.\\
$$P = \begin{bmatrix}
    1/3 & 2/3 & 0 \\
    0 & 1/3 & 2/3 \\
    2/3 & 0 & 1/3
\end{bmatrix}$$
This is doubly stochastic.\\
Argue that $Q$ is the same as $P^T$\\
Then show $Q \neq P$ because $P$ is not symmetric.\\



\end{document} 