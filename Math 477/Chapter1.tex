\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{cancel}

\usepackage{graphicx}


\setlength\parindent{0pt}

\author{Pranav Tikkawar}
\title{Math Theory of Probability}

\begin{document}
\maketitle
\tableofcontents
\section{Chapter 1: Combinatorial Analysis}
\subsection*{5/28}
\textbf{Basic Principle of Counting.}\\
Suppose that 2 experiments are to be preformed. Then if exp 1 can result in any one of $n_1$ possible outcomes and for each of these outcomes, exp 2 can result in any one of $n_2$ possible outcomes, then the total number of possible outcomes for the 2 experiments is $n_1 \cdot n_2$.\\

\textbf{Permutations.}\\
How many ways are there of arranging $n$ distinct things?\\
There are $n$ ways to choose the first thing, $n-1$ ways to choose the second thing, $n-2$ ways to choose the third thing, and so on.\\
Thus, the total number of ways of arranging $n$ distinct things is $n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 2 \cdot 1 = n!$\\

\textbf{Permutations with repeats.}\\
$$\frac{n!}{n_1! \cdot n_2! \cdot \ldots \cdot n_r!} $$
different permutation of $n$ objects which any arbitrary $n_i$ are alike.\\

\textbf{Combinations.}\\
$$ \binom{n}{r} = \frac{n!}{(n-r)!r!}$$
How many ways are there of choosing $r$ things from $n$ distinct things?

\subsection*{5/29}
\textbf{Example 4c:} $n$ items $m$ are dysfunctional and $n-m$ are functional. What is the probability that no two dysfunctional items are adjacent?\\
\textbf{Sol:} There are $\binom{n-m+1}{m}$ ways. If we think of the functional (plus one for the before spot) we can put the dysfunctional items in. Thus resulting in $\binom{n-m+1}{m}$ ways.\\

\textbf{Question} Prove that $\binom{n}{r} = \binom{n-1}{r-1} + \binom{n-1}{r}$\\
It is shown is pascal's triangle.\\
Since the left is the number of Combinations of $n$ things taken $r$ at a time, and the right is the number of Combinations of $n-1$ things taken $r-1$ at a time and $r$ at a time.\\ 
Thus the right side is the number of Combinations in which A is included and the number of Combinations in which A is not included.\\

\textbf{Binomial Theorum}\\
$$ (x+y)^n = \sum_{r=0}^{n} \binom{n}{r} x^{n-r}y^r$$
This gives the coefficients of the expansion of $(x+y)^n$\\

\textbf{Multinomial Theorum}\\
$$ (x_1 + x_2 + \ldots + x_k)^n = \sum_{r_1 + r_2 + \ldots + r_k = n} \frac{n!}{r_1!r_2! \ldots r_k!} x_1^{r_1}x_2^{r_2} \ldots x_k^{r_k}$$
This gives the coefficients of the expansion of $(x_1 + x_2 + \ldots + x_k)^n$\\

\textbf{Something cool} $\binom{n}{r} \binom{r}{k} = \frac{n!}{r!k!(n-r-k)!}$\\

\textbf{Example 5:} 8 players, 4 matches (identitcal) played of 2 players. How many ways can the matches be played?\\
\textbf{Sol:} There are $\frac{8!}{2!2!2!2!4!} = 105$ ways.\\
How many ways can people win?\\
16 ways.\\

\textbf{Class Activity:} Consider the equation $x_1 + x_2 + \ldots + x_r = n$ where each $x_i$ is non-negative. How many possible solutions are there to this equation. 

\section{Chapter 2: Axioms of Probability}
\subsection*{5/30}
\textbf{Axioms of probability}\\
The probability of Something happening is the number of ways the thing happens dived by the possible outcomes.\\
\textbf{Axiom 1:} $0 \leq P(A) \leq 1$\\
\textbf{Axiom 2:} $P(S) = 1$ where $S$ is whole same space\\
\textbf{Axiom 3:} If $A_1, A_2, \ldots$ are mutually exclusive events, then $P(A_1 \cup A_2 \cup \ldots) = P(A_1) + P(A_2) + \ldots$\\

\subsection*{6/3}
$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$\\

\subsection*{6/4}
\textbf{n ppl who throw the hats in what is the probablity that no one gets their own hat?}\\
$\frac{1}{e}$ as $n \to \infty$\\
We want probability of $A_1 \cap A_2 \cap \ldots \cap A_n$\\
$P(A_1 \cap A_2 \cap \ldots \cap A_n) = 1 - P(A_1^c \cup A_2^c \cup \ldots \cup A_n^c)$\\
$P(A_1^c \cup A_2^c \cup \ldots \cup A_n^c) = 1 - P(A_1^c) - P(A_2^c) - \ldots - P(A_n^c) + P(A_1^c \cap A_2^c) + \ldots$\\
With a buch of work we get:
$$ 1 - \sum_{i=1}^{n} (-1)^{i+1} \frac{(x-i)!}{x!}\binom{x}{i}$$
This results in $1 - \frac{1}{2!} + \frac{1}{3!} - \frac{1}{4!} + \ldots$\\

\textbf{Example 50} on page 42 of 8th edition.\\

\section{Chapter 3: Conditional Probability and Independence}
\subsection*{6/6}
$$P(A \cap B) = P(A)P(B|A) = P(A)P(B)$$
iff E and F are independent.\\
\subsection*{6/10}
\textbf{Bayes formula}\\
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
\textbf{Odds}\\
$$\frac{P(A)}{P(A^c)}$$
\textbf{New Odds}\\
$$\frac{P(A|B)}{P(A^c|B)} = \frac{P(B|A) \cdot P(A)}{P(B|A^c) \cdot P(A^c) }$$
\section{Chapter 4: Random Variables}
\subsection*{6/20}
\textbf{Random Variable Info}\\
\textbf{Binomial Random Variable}\\
$P[x=n] = \binom{N}{n}p^n(1-p)^{N-n}$\\
$\mathbb{E}[x] = Np$\\
$Var[x] = Np(1-p)$\\
\textbf{Poisson Random Variable}\\
$P[x=n] = \frac{e^{-\lambda}\lambda^n}{n!}$\\
$\mathbb{E}[x] = \lambda$\\
$Var[x] = \lambda$\\
\textbf{Geometric Random Variable}\\
$P[x=n] = (1-p)^{n-1}p$\\
$\mathbb{E}[x] = \frac{1}{p}$\\
$Var[x] = \frac{1-p}{p^2}$\\
\textbf{Negative Binomial Random Variable}\\
$P[x=n] = \binom{n-1}{r-1}p^r(1-p)^{n-r}$\\
$\mathbb{E}[x] = \frac{r}{p}$\\
$Var[x] = \frac{r(1-p)}{p^2}$\\
\textbf{Hypergeometric Random Variable}\\
$P[x=n] = \frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}$\\
$\mathbb{E}[x] = \frac{Mk}{N}$\\
$Var[x] = \frac{N-M}{N-1}k\frac{M}{N}\frac{N-k}{N-1}$\\
$Var[x] = np(1-p)(1-\frac{n-1}{N-1})$\\
\textbf{Zeta Random Variable}\\
$P[x=k] = \frac{c}{k^{\alpha + 1}}$\\
$\mathbb{E}[x] = \frac{c}{\alpha - 1}$\\
$Var[x] = \frac{c^2}{(\alpha - 1)^2(\alpha - 2)}$\\


\section{Chapter 5: Continuous Random Variables}
$pdf[a \leq x \leq b] = \int_{a}^{b} f(x)dx$\\
Where $f(x)$ is the probability density function.\\
A Cumulative Distribution Function is $cdf(x) = P[X \leq x]$\\
Also $cdf(x) = \int_{-\infty}^{x} f(x)dx$\\
\textbf{Gaussian Random Variable}\\
$P[x=n] = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$\\
$\mathbb{E}[x] = \mu$\\
$Var[x] = \sigma^2$\\
\textbf{Exponential Random Variable}\\
$P[x=n] = \lambda e^{-\lambda x}$\\
$\mathbb{E}[x] = \frac{1}{\lambda}$\\
$Var[x] = \frac{1}{\lambda^2}$\\
A non negative Random variable c is memmoryless if $P[x>s+t | x > s] = P[x>t]$
\textbf{Gamma Random Variable}\\
$P[x=n] = \frac{\lambda^{\alpha}x^{\alpha - 1}e^{-\lambda x}}{(\alpha - 1)!}$\\
$P[x=n] = \frac{\lambda^{\alpha}x^{\alpha - 1}e^{-\lambda x}}{\Gamma(\alpha)}$\\
$\mathbb{E}[x] = \frac{\alpha}{\lambda}$\\
$Var[x] = \frac{\alpha}{\lambda^2}$\\
\textbf{Weibull Random Variable}\\
$P[x=n] = \frac{\alpha}{\beta}(\frac{x}{\beta})^{\alpha - 1}e^{-(\frac{x}{\beta})^{\alpha}}$\\
$\mathbb{E}[x] = \beta \Gamma(1 + \frac{1}{\alpha})$\\
$Var[x] = \beta^2[\Gamma(1 + \frac{2}{\alpha}) - \Gamma^2(1 + \frac{1}{\alpha})]$\\


\end{document}