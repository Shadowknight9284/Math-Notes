\documentclass[answers,12pt,addpoints]{exam}
\usepackage{import}

\import{C:/Users/prana/OneDrive/Desktop/MathNotes}{style.tex}

% Header
\newcommand{\name}{Pranav Tikkawar}
\newcommand{\course}{01:XXX:XXX}
\newcommand{\assignment}{Homework n}
\author{\name}
\title{\course \ - \assignment}

\begin{document}
\maketitle
\section{Chapter 11: Confidence Intervals}
Given a $\alpha$ such that $0 < \alpha < 1$, an \underline{Interval Estimation} stratgey provides two statistics (r.v) $L$ and $R$ s.t $P(L < \theta < R) = 1 - \alpha$. The interval $[L, R]$ is called a $(1-\alpha)\%$ confidence interval for $\theta$.\\
We can say that if you repeat the expirment $N$ times and gte $N$ intervals, then $(1-\alpha)\%$ of the intervals will contain the true value of $\theta$.\\
\begin{definition}[Confidence Interval]
    A confidence interval with (1-$\alpha$) confidence level are two statistics $L$ and $R$ such that $P(L < \theta < R) = 1 - \alpha$.
\end{definition}
\begin{remark}
    If someone says after an expirment that $2 < \lambda < 2.1$ with 90\% confidence, it means that on average that 90\% of the intervals will contain the true value of $\lambda$.
\end{remark}
We want CI to be symetric about $\bar{X}$ 




\newpage
CI so far: 
$N(\mu, \sigma^2)$ population
\begin{enumerate}
    \item $\sigma^2$ known: $\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$
    \item $\sigma^2$ unknown: $\bar{X} \pm t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}$
\end{enumerate}
We can think of $z_{\alpha/2}$ in the normal curve as the shaded area in the tails.\\\\
\textbf{New Context:}
Two pops $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$ and sample from both \\
$n_1$ and $n_2$ from each
$$X_{11} \ldots X_{1n_1} \sim N(\mu_1, \sigma_1^2)$$
$$X_{21} \ldots X_{2n_2} \sim N(\mu_2, \sigma_2^2)$$
These are independent but not necessarily identically distributed.\\
Let $\bar{X}_1$ and $\bar{X}_2$ be the sample means and $S_1^2$ and $S_2^2$ be the sample variances.\\
Want CI for $\mu_1 - \mu_2$\\
\textbf{Case 1: } $\sigma_1^2$ and $\sigma_2^2$ are known.\\
We can use point estimators:
$$\bar{X}_1 - \bar{X}_2 \sim N(\mu_1 - \mu_2 , \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})$$
$$\bar{X}_1 - \bar{X}_2 \pm z_{\alpha/2} \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
\textbf{Case 2: } $\sigma_1^2$ and $\sigma_2^2$ are unknown. but $\sigma_1^2 = \sigma_2^2$\\
we can define a pooled sample variance:
$$S_p^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$
Also 
$$ \frac{(n_1 + n_2 - 2) S_p^2}{\sigma^2} = \frac{(n_1 - 1)S_1^2}{\sigma^2} + \frac{(n_2 - 1)S_2^2}{\sigma^2} \sim \chi^2_{n_1 + n_2 -2}$$
\begin{remark}
    This is a weighted average of the sample variances. with weights $n_1 - 1$ and $n_2 - 1$
\end{remark}
\begin{remark}
    $$S_p^2 := \frac{\sum^{n_1} (X_{1i} - \bar{X_1})^2 + \sum^{n_2}( X_{2i} + \bar{X_2} )^2}{n_1 + n_2 - 2}$$
    $$ \sim \chi^2_{n_1 + n_2 - 2}$$
\end{remark}
\begin{remark}
    Then the r.v $T$ is 
    $$T = \frac{\bar{X}_1 - \bar{X}_2 - (\mu_1 - \mu_2)}{S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}$$
\end{remark}
Consider making a CI for $\sigma^2$\\
\begin{remark}
    We know that $S^2$ is a point estimator for $\sigma^2$ \\
    $$K = \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$$
    Consider the graph of the $\chi^2$ distribution. It is definied from $0, \infty$ and we can trap an $\alpha$ area in the tails.\\
    We can call this $\left( \chi^2_{n-1} \right)_{\alpha/2}$ and $\left( \chi^2_{n-1} \right)_{1 - (\alpha/2)}$\\
    Notice the non symmetry of the $\chi^2$ distribution.\\
    We can say that $P\left( \left( \chi^2_{n-1} \right)_{\alpha/2} < K < \left( \chi^2_{n-1} \right)_{1 - (\alpha/2)} \right) = 1 - \alpha$\\
    In a more insightful form we have
    $$ P\left( \frac{(n-1) S^2}{\chi^2_{n-1, \alpha/2}} < \sigma^2 < \frac{(n-1) S^2}{\chi^2_{n-1, 1 - \alpha/2}}\right) = 1 - \alpha$$
\end{remark}
\begin{definition}[F distribution]
    F distribution with $\nu_1 > 0$ and $\nu_2 > 0$ degrees of freedom is defined by
    $$F = \frac{\frac{U}{\nu_1}}{\frac{V}{\nu_2}}$$
    where $U \sim \chi^2_{\nu_1}$ and $V \sim \chi^2_{\nu_2}$\\
    ie $F = \frac{U\nu_2}{V\nu_1}$
    \begin{remark}
        Order is important
    \end{remark}
\end{definition}
\begin{example}
    $\setof{X_i}_{i \in range(5)}$ independent standard normal r.v.\\
    Then $\frac{\alpha (Z_1^2 + Z_2^2)}{Z_3^2 + Z_4^2 + Z_5^2} \sim F_{2,3}$ where $\alpha = 3/2$\\
    This is due to the fact that $Z_i^2 \sim \chi^2_1$ and $\sum_i^n Z_i^2 \sim \chi^2_n$
\end{example}
\begin{theorem}
    PDF of $f_{\nu_1, \nu_2}$ is
    $$g(x) = \frac{\Gamma\left( \frac{\nu_1 + \nu_2}{2} \right)}{\Gamma\left( \frac{\nu_1}{2} \right) \Gamma\left( \frac{\nu_2}{2} \right)} \left( \frac{\nu_1}{\nu_2} \right)^{\nu_1/2} x^{\frac{\nu_1}{2} - 1} \left( 1 + \frac{\nu_1}{\nu_2} x \right)^{-\frac{\nu_1 + \nu_2}{2}}$$ 
\end{theorem}
If we consider two normal pops $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$ if
$$ \frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2_{n_1 - 1} and \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2_{n_2 - 1}$$
then
$$ \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} \sim F_{n_1 - 1, n_2 - 1}$$
This is useful for a CI for ratio of variances.\\

\section*{Hypothesis Testing}
\begin{definition}[Hypothesis Testing]
    It is an assertion about the theoretical distribution. for example about parameters.
    \begin{example}
        In a coin toss expiriment C $Ber(p)$ "coin is unfair": $p \neq 0.5$\\
    \end{example}
    It is simple if it completely determines the distribution\\
    It is not simple it is composite
\end{definition}
\begin{example}
    Coin is fair (ie $p = 0.5$)\\
    is a simple hypothesis
\end{example}
\begin{definition}[Statisical Test]
    it is a criterion used to ecide whether to reject (or acept) one hypothesis called the null hypothesis $H_0$ in favor of an alternative hypothesis $H_1$.
\end{definition}

\section*{Chapter 12}
\begin{example}
    $N( \mu , \sigma^2 = 4)$ sample to get $\setof{X_i}$ want a test to find $\bar{X} <C$\\
    $H_0: \mu = 4$\\
    $H_1: \mu = 1$\\
    $\alpha = P(\text{reject } H_0 | H_0 \text{ is true})$\\
    $\beta = P(\text{accept } H_0 | H_1 \text{ is true})$\\
    We found that if $\alpha$ is small, then $\beta$ is not if n is fixed. \\
    Increasing $n$ will decrease $\beta$\\
\end{example}
\begin{example}
    Consdier $Exp(\lambda)$ 
    $H_0: \lambda = 4$\\
    $H_1: \lambda = 1$\\
    Suppose we get $\setof{X_i}$\\
    $\bar{X} < C$\\
    Find $\alpha$ and $\beta$ for a fixed $C > 0$ Assume sample size is 1\\
    PDF is $f(x) =e^{-x/ \lambda} / \lambda $\\
    Thus $\alpha = P(\bar{X} < C | \lambda = 4)$\\
    $\beta = P(\bar{X} > C | \lambda = 1)$\\
    We can use the CDF of the exponential distribution to find these values.\\
    
\end{example}
\textbf{Context:}\\
Given pdf type, parameters unnown: \\
Two statements: $H_0$ and $H_1$\\
Base on sample decide among these two.\\
Test spifies criterion on sample under which $H_0$ is rejected.\\
Where if Outcomes falls $H_0$ is rejected\\
This reigon is called the critical region.\\
With error $\alpha = P(\text{reject } H_0 | H_0 \text{ is true})$\\
$\alpha$ is the area of the critical region or level of significance.\\
Now define $1-\beta$ as the power of the test.\\
\begin{definition}[Power of a Test]
    The power of a test is the probability of rejecting $H_0$ when $H_1$ is true.\\
    ie $1 - \beta = P(\text{reject } H_0 | H_1 \text{ is true})$
\end{definition}
If we have multiple $x_i$ we use jpdf. \\
\begin{definition}[Neyman-Pearson Lemma]
    Suppose $H_0: \theta = \theta_0$ and $H_1: \theta = \theta_1$\\
    These are both simple\\
    Let $L_0(x_1 ... x_n) = f(x_1 ... x_n | \theta_0)$ ie the jpf of $\theta = \theta_0$\\
    Similarly $L_1(x_1 ... x_n) = f(x_1 ... x_n | \theta_1)$ ie the jpf of $\theta = \theta_1$\\
    Suppose $C$ is a reigon in the coutocme space where $L_0/L_1 \leq k$\\
    But outside $C$ $L_0/L_1 > k$\\
    Then the test that rejects $H_0$ if $L_0/L_1 \leq k$ is the most powerful test of size $\alpha$
\end{definition}
\begin{example}
    $N(\mu, \sigma^2)$\\
    $H_0: \mu = 1$\\
    $H_1: \mu = 3$\\
    Find max power crit region\\
    $L_0 = f(x_1 ... x_n | \mu = 1)$\\
    $L_1 = f(x_1 ... x_n | \mu = 3)$\\
    $L_0/L_1 = \frac{e^{-\frac{1}{2\sigma^2} \sum_i^n (x_i - 1)^2}}{e^{-\frac{1}{2\sigma^2} \sum_i^n (x_i - 3)^2}} = e^{-\frac{1}{2}\sum_i^n\left( (x_i-1)^2 - (x_i-3)^2 \right)}$\\
    $$ e^{-\sum_i^n \left( 2x_i - 4 \right)} \leq k$$
\end{example}
\begin{definition}[Likelihood Ratio Test]
    Last class considered the NPL for simple hypothesis.\\
    $H_0: \theta = \theta_0$\\
    $H_1: \theta = \theta_1$\\
    $L_0, L_1$ are functions of $x_1 ... x_n$\\
    $C = \setof{x_1 ... x_n | L_0/L_1 \leq k}$\\
    For some $k$\\
    aka it is smaller up to a scaling factor.\\
    Then $C$ is the Critical region with max power $(1- \beta)$ among all tests of same $\alpha$\\
\end{definition}
We can consider $w, w'$ as two non-intersection subsets of the parameter space\\
Consider that $H_0: \theta \in w$ and $H_1: \theta \in w'$\\
\begin{example}
    $N(\mu, \sigma^2 = 1)$\\
    $H_0: \mu = 2$\\
    $H_1: \mu \neq 2$\\
    Clealry this is simple and composite hypothesis respectively.\\
    $w = \setof{2}$ and $w' = R \setminus \setof{2}$\\
\end{example}
"Meaningful" tests Reject $H_0$ iff the Likelihood that $\Theta$ is in $w$ is small.\\
We can see that Likelihood is now 
$$L_w ( x_1 ... x_n) = \max_{\theta \in w} f(x_1 ... x_n | \theta)$$
$L_{\Omega} = \max_{\theta \in \Omega} f(x_1 ... x_n | \theta)$\\
Let $\Lambda = \frac{L_w}{L_{\Omega}}$\\
\begin{remark}
    $L_{\Omega}$ is at least as large as $L_w$\\
    Thus $\Lambda$ is in $0,1$
\end{remark}
\begin{definition}[Likelihood Ratio Test]
   Reject $H_0$ if $\Lambda \leq k$\\
   where $k$ is fixed number between 0 and 1\\
\end{definition}
\begin{example}
    $N(\mu, \sigma^1 = 1)$
    $H_0: \mu = 2$\\
    $H_1: \mu \neq 2$\\
    Now when we have $L(x_1 ... x_n | \mu)$ for this to maximize we take a same method as MLE.
    We see that $\mu = \mu_{MLE} = \bar{X}$\\
    So for Likelihood ratio statistic
    $$ \Lambda = \frac{L_w}{L_{\Omega}} $$
    $$ = e^{-\frac{1}{2} \sum_i^n (x_i - 2)^2} / e^{-\frac{1}{2} \sum_i^n (x_i - \bar{X})^2}$$
    Now we can simplify since the function is monotone inc.\\
    thus 
    $$ln(\Lambda) = \frac{1}{2} \sum_i^n (x_i - \bar{X})^2 - \frac{1}{2} \sum_i^n (x_i - 2)^2$$
    Doing some math we get 
    $$ |\bar{x} - 2| \geq  \sqrt{\frac{-2ln(k)}{n}}$$
    Rename constants without $x_i$ as $\tilde{k}$\\
    $$ |\bar{x} - 2| \geq \tilde{k}$$
\end{example}

\newpage
\section*{Review}
\begin{definition}[F distribution]
    F dist with $\nu_1 > 0$ and $\nu_2 > 0$ degrees of freedom is defined by
    $$F = \frac{\frac{U}{\nu_1}}{\frac{V}{\nu_2}}$$
    where $U \sim \chi^2_{\nu_1}$ and $V \sim \chi^2_{\nu_2}$\\
    If rv $F \sim f_{\nu_1, \nu_2}$ then the rv $\frac{1}{F} = f_{\nu_2, \nu_1}$ ie indices are swapped.\\
    Now consider $f_{\nu_1, \nu_2, \alpha} $ which is $P(F > f_{\nu_1, \nu_2, \alpha}) = \alpha$\\
    Thus $P(F \geq f_{\nu_1, \nu_2, 1 - \alpha}) = \alpha \leftrightarrow P(\frac{1}{F} \leq \frac{1}{f_{\nu_1, \nu_2, \alpha}}) = \alpha$\\
    $$ P(\frac{1}{F} \geq \frac{1}{f_{\nu_1, \nu_2,\alpha}}) = 1- \alpha$$
    Since $\frac{1}{F} \sim f_{\nu_2, \nu_1}$
    $$ f_{\nu_2, \nu_1, 1 - \alpha} = \frac{1}{f_{\nu_1, \nu_2, \alpha}}$$
\end{definition}

\begin{definition}[Test of Significance]
    $H_0: \theta = \theta_0$\\ 
    $H_1: \theta \neq \theta_0$ or $H_1: \theta > \theta_0$ or $H_1: \theta < \theta_0$\\
    Idea of test: given $\alpha$, $\beta$ not specified. \\
    Outcome: Reject $H_0$ or Fail to reject $H_0$\\ 
    Test: Compare the value of a test stat with a fixed value that depends on dist $\propto \alpha$
\end{definition}



\end{document}