\documentclass[answers,12pt,addpoints]{exam} 
\usepackage{import}

\import{C:/Users/prana/OneDrive/Desktop/MathNotes}{style.tex}

% Header
\newcommand{\name}{Pranav Tikkawar}
\newcommand{\course}{16:960:665}
\newcommand{\assignment}{Syllabus}
\author{\name}
\title{\course \ - \assignment}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section*{Syllabus}

Time Series: Theory and Methods. Brockwell and Davis\\
Asymtotic Theory of Weakly dependent Random Process\\
Martingale Limit Theory\\

Durret - Probability Theory and Examples\\

\section*{Questions}
Ask what I need to get and review before classes start\\
Measure Theory: not hardcore\\ 
look into textbooks and ergodic theory\\
Ask the professors of the classes to audit \\
What is the $X.(\omega)$ noations

\section*{Acronyms}
R.V. - Random Variable\\
S.P. - Stochastic Process\\
fn - Function\\
dist - Distribution\\
G.P. - Gaussian Process\\
iid - independent and identically distributed\\
a.s. - Almost Surely\\
w.p 1 - with probability 1\\

\section{Notes}
\subsection{9/2/2025 Lecture 1}
\textbf{We use Stochastic Process to model time series data}
\begin{definition}[Stochastic Process]
    A stochastic process is a family of random variables $\{X_t : t \in \mathcal{T}\}$ defined on a common probability space $(\Omega, \mathcal{F}, \Prob)$.\\
    $\mathcal{T} = \mathbb{N}, \mathbb{Z}$ Discrete Time\\
    $\mathcal{T} = \mathbb{R}$ Continuous Time (not focusing on this)\\
    $\mathcal{T} \subseteq \mathbb{R}^n$ Geospatial, with location and time, (not focusing on this) \\
    $\mathcal{T} \subseteq \mathbb{S}^3$ Unit Sphere w/Geophysics.
\end{definition}
\begin{definition}[Realization of a S.P.]
    The functions $\{X.(\omega), \omega \in \Omega \} (\mathcal{T} \to \mathbb{R})$ are realizations or sample passes of the process.
    \begin{itemize}
        \item Fix $t$, $X_t$ is a fn of $\Omega$
        \item Fix an outcome $\omega \in \Omega$, $X.(\omega)$ is a fn on $\mathcal{T}$
        \item The time series we observe is a realization of the S.P.
        \item Conventionally the observed time series is indexed by $\{1,2, \ldots, n\}$ ie $\{X_1, X_2, \ldots, X_n\}$ (known as the lens/sample size)
    \end{itemize}
\end{definition}
\begin{example}[1.2.1 from book]
    Suppose $A \geq 0$ is a R.V and given by $\Theta \sim Uniform(0,2\pi)$. and they are independent. and $v > 0$ is a known constant\\
    Then $X_t = A \cos(vt + \Theta), t \in \mathbb{Z}$\\
    Fore every $\omega \in \Omega$, $A(\omega), \Theta(\omega)$ are fixed\\
    $X_t(\omega) = A(\omega) \cos(vt + \Theta(\omega))$\\
    $A$ determines the amplitude and $\Theta$ determines the phase. \\
    What we do is we take a model, and have the data as a realization, and solve the inverse problem of determining the parameters of the model.
\end{example}
\begin{example}[1.2.2 from the book] 
    Consider $X_1, X_2, X_3, \ldots$ are IID and take value $1, -1$ with probability $1/2$\\
    Im considering to use some binomal theorem thing...
\end{example}
\begin{example}[1.2.3 from the book]
    Suppose $X_t$ coming from prior question.\\
    $S_t = \sum_{i = 1}^{t} X_i = X_1 + X_2 + \ldots + X_t$
    $S_t: t \in \mathbb{N}$ is a S.P. called a simple symmetric random walk\\
    Consider a man in 1D who starts at 0, and takes a random draw to walk left or right. The path of this miserable guys is $S_t$\\
    The realization is a plot of $S_t(\omega)$ against $t$.
\end{example}
\begin{definition}[The Distribution of a Stochastic Process]
    Let $\mathcal{I}$ be the collection of all tuples $\{\mathbf{t} = (t_1, t_2, \ldots, t_n)' \in \mathcal{T}, t_1 < t_2 < \ldots < t_n\}$ The finite dimensional dist. fns of $\{X_t, t \in \mathcal{T}\}$ are the collection of fns $\{ F_t(\cdot) \mathbf{t} \in \mathcal{I} \}$ where
    $$ F_{\mathbf{t}}(\mathbf{x}) =  P(X_{\mathbf{t_1}} = x_1, X_{\mathbf{t_2}} = x_2, \ldots, X_{\mathbf{t_n}} = x_n)$$
    $$ \mathbf{x} = (x_1, x_2, \ldots, x_n)' \in \mathbb{R}^n$$
    ie $F_{\mathbf{t}}(\mathbf{x})$ is the joint distribution of the process of the R.V. $\mathbf{x}$.\\
\end{definition}
\begin{theorem}[Kolmogrov (consistincy) Theorem]
    The prob. distribution fns $\{ F_{\mathbf{t}}(\cdot) : \mathbf{t} \in \mathcal{I} \}$ are the distribution functions of some S.P. $\iff$ for any $n \in \mathbb{N}$, $\mathbf{t} = (t_1, t_2, \ldots, t_n)' \in \mathcal{T}$ and $1 \leq i \leq n$
    $$ \lim_{x_i \to \infty} F_{\mathbf{t}}(\mathbf{x}) = F_{\mathbf{t_i}}(\mathbf{x_i})$$
    Where $\mathbf{x} = (x_1, x_2, \ldots, x_n)'$,\\
    $\mathbf{t_i} = (t_1, t_2, \ldots, t_{i-1}, t_{i+1}, \ldots, t_n)'$ and $\mathbf{x_i} = (x_1, x_2, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)'$ \\
    essentially the $i$ are the missing ones
    \begin{align*}
        F(x_1, x_2) = \Prob (X_1 \leq x_1, X_2 \leq x_2)\\
        \lim_{x_2 \to \infty} F(x_1, x_2) = \Prob(X_1 \leq x_1)
    \end{align*}
    "https://en.wikipedia.org/wiki/Kolmogorov\_extension\_theorem"\\
    We essentially only need to specify the consistency of the finite dimensional distributions to define a S.P.
\end{theorem}
\subsection{9/9/2025 Lecture 2}
\begin{definition}[Autocovariance function]
    If ${X_t, t \in \mathcal{T}}$ is a S.P. s.t $E(X_t^2) < \infty$, then for every $t \in \mathcal{T}$ the the autocovariance function is defined as
    $$ \gamma_x(r, s) = Cov(X_r, X_s), r,s \in \mathcal{T}$$
\end{definition}
\begin{definition}[Autocorrelation function]
    If ${X_t, t \in \mathcal{T}}$ is a S.P. s.t $E(X_t^2) < \infty$, then for every $t \in \mathcal{T}$ the the autocorrelation function is defined as
    $$ \rho_x(r, s) = Corr(X_r, X_s) = \frac{\gamma_x(r,s)}{\sqrt{\gamma_x(r,r)\gamma_x(s,s)}}, r,s \in \mathcal{T}$$
\end{definition}
\begin{definition}[Stationary S.P]
    A stochastic process ${X_t, t \in \mathcal{T}}$ is said to be stationary 
    \begin{itemize}
        \item $E(X_t^2) < \infty$ for all $t \in \mathcal{T}$
        \item $E(X_t) = \mu$ for all $t \in \mathcal{T}$
        \item $\gamma_x(r,s) = \gamma_x(r+h, s+h)$ for all $r,s,h \in \mathcal{T}$
    \end{itemize}
    Weakly Stationary/Covariance Stationary/Wide Sense Stationary/Second Order Stationary\\
    \fbox{\textbf{ASK: If our $\mathcal{T}$ is a non convex set, does this still hold?}}\\
    Also if ${X_t}$ is stationary, then $\gamma_x(r,s) = \gamma_x(0, s-r) = \gamma_x(s-r)$ ie we can define the autocovariance as a fn of the one variable: the lag $h = s-r$\\
    Similarly $\rho_x(h) = \frac{\gamma_x(h)}{\gamma_x(0)}$
\end{definition}
\begin{definition}[Strict Stationarity]
    A stochastic process ${X_t, t \in \mathcal{T}}$ is said to be strictly stationary if for every $n \in \mathbb{N}$, $t_1, t_2, \ldots, t_n \in \mathcal{T}$ and $h \in \mathcal{T}$ the random vectors $(X_{t_1}, X_{t_2}, \ldots, X_{t_n})'$ and $(X_{t_1 + h}, X_{t_2 + h}, \ldots, X_{t_n + h})'$ have the same distribution.\\
    ie the finite dimensional distributions are shift invariant.\\
    If Strict Stationarity with finite second moments $\implies$ Weak Stationarity.
\end{definition}
\begin{definition}[Gaussian Time Series (S.P)]
    A Gaussian S.P. is a S.P. ${X_t, t \in \mathcal{T}}$ if all the finite dimensional distributions fns of $\{X_t\}$ are multivariate normal. \\
    ie for every $n \in \mathbb{N}$ and $t_1, t_2, \ldots, t_n \in \mathcal{T}$ the random vector $(X_{t_1}, X_{t_2}, \ldots, X_{t_n})'$ has a multivariate normal distribution.
    - IF a G.P. is stationary, then it is strictly stationary.
\end{definition}
\begin{definition}[Stationarity of IID]
    IID variables are strictly stationary.
\end{definition}
\begin{definition}[White Noise]
    A S.P. ${X_t}$ is said to be white noise if can also be written as $WN(0, \sigma^2)$ 
    \begin{itemize}
        \item $E(X_t) = 0$ for all $t$
        \item $Var(X_t) = \sigma^2 < \infty$ for all $t$
        \item $Cov(X_t, X_s) = 0$ for all $t \neq s$
    \end{itemize}
    It is a weakly stationary S.P.
\end{definition}
\begin{example}[Example of White Noise not Strictly Stationary]
    Let $X_t$ with $t = even$ be $N(0,1)$ and $X_t$ with $t = odd$ be $Rademacher(0,1)$\\
    Then $X_t$ is white noise but not strictly stationary.
\end{example}
\begin{example}[1.3.1]
    $X_t = A \cos(\Theta t) + B \sin(\Theta t)$ where $E(A) = E(B) = 0$, $Var(A) = Var(B) = 1$, $Cov(A,B) = 0$\\
    \begin{itemize}
        \item $E(X_t) = 0$
        \item $Var(X_t) = E(A^2 \cos^2(\Theta t) + B^2 \sin^2(\Theta t)) = \cos^2(\Theta t) + \sin^2(\Theta t) = 1$
        \item $Cov(X_t, X_s) = E(X_t X_s) = E[(A \cos(\Theta t) + B \sin(\Theta t))(A \cos(\Theta s) + B \sin(\Theta s))] = E[A^2] \cos(\Theta t) \cos(\Theta s) + E[B^2] \sin(\Theta t) \sin(\Theta s) = \cos(\Theta t) \cos(\Theta s) + \sin(\Theta t) \sin(\Theta s) = \cos(\Theta(t-s))$
    \end{itemize}
    Note that the $Cov(X_t, X_s)$ is only a fn of $t-s$\\
    Thus $X_t$ is weakly stationary.
\end{example}
\begin{example}[1.3.2]
    Let $Z_t, t \in \mathbb{Z}$ be IID(0, $\sigma^2$)\\
    $X_t = Z_t + \Theta Z_{t-1}$
    \begin{itemize}
        \item $E(X_t) = 0$
        \item $Var(X_t) = Var(Z_t) + \Theta^2 Var(Z_{t-1}) = (1 + \Theta^2)\sigma^2$
        \item $Cov(X_t, X_s) = E(X_t X_s) = E[(Z_t + \Theta Z_{t-1})(Z_s + \Theta Z_{s-1})] = \Theta \sigma^2$ if $|t-s| = 1$, $(1 + \Theta^2)\sigma^2$ if $t=s$, $0$ otherwise
    \end{itemize}
    Thus $X_t$ is weakly stationary.
\end{example}
\begin{example}[1.3.4]
    Assume ${X_t}$ is IID(0, $\sigma^2$)\\
    $S_t = X_1 + X_2 + \ldots + X_t$ $t \geq 1$
    \begin{itemize}
        \item $E(S_t) = 0$
        \item $Var(S_t) = t \sigma^2$ Not constant 
        \item $\cov(S_r, S_t) = E(S_r S_t) = r \sigma^2$ WLOG $r \leq t$
        \item $Cov(S_r, S_t) = (r \wedge t) \sigma^2$
    \end{itemize}
\end{example}
\begin{proposition}[1.5.1]
    Suppose ${X_t}$ is weakly stationary with $\gamma_x(h), \rho_x(h)$ as the autocovariance and autocorrelation fns. Then
    \begin{itemize}
        \item $\gamma_x(0) \geq 0$
        \item $|\gamma_x(h)| \leq \gamma_x(0)$ for all $h \in \mathcal{T}$
        \item $\gamma_x(h) = \gamma_x(-h)$ for all $h \in \mathcal{T}$
    \end{itemize}
\end{proposition}
\begin{remark}[Some Statistics...]
    Observe $\{X_t\}, t = 1, 2, \ldots, n$
    Want to estimate $\mu, \gamma(0), \gamma(1), \ldots, \gamma(n-1)$
    \begin{align*}
        \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} X_i := \bar{X}\\
        \hat{\gamma}(0) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2\\
        \hat{\gamma}(1) = \frac{1}{n} \sum_{i=1}^{n-1} (X_i - \bar{X})(X_{i+1} - \bar{X})\\
        \hat{\gamma}(h) = \frac{1}{n} \sum_{i=1}^{n-h} (X_i - \bar{X})(X_{i+h} - \bar{X})\\
    \end{align*}
    The reason why we divide by $n$ we want to shrink it. intuition is that we want to make autocorrelation smaller as $n$ increases.
\end{remark}
\subsection{9/11/2025 Lecture 3}
\begin{remark}[Matrix Form of Autocovariance]
    Observe $X_1, X_2, \ldots, X_n$ \\
    $\hat{\gamma}(h) = \frac{1}{n} \sum_{i=1}^{n-h} (X_i - \bar{X})(X_{i+h} - \bar{X})$.\\
    $$ \Gamma_n = \cov\begin{bmatrix}
        X_1\\
        X_2\\
        \vdots\\
        X_n
    \end{bmatrix} = \begin{bmatrix}
        \gamma_x(0) & \gamma_x(1) & \gamma_x(2) & \ldots & \gamma_x(n-1)\\
        \gamma_x(1) & \gamma_x(0) & \gamma_x(1) & \ldots & \gamma_x(n-2)\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        \gamma_x(n-1) & \gamma_x(n-2) & \gamma_x(n-3) & \ldots & \gamma_x(0)
    \end{bmatrix}$$
    This is a Toeplitz matrix. ie constant along the diagonals. It is also positive semidefinite. ie $a' \Gamma_n a \geq 0$ for all $a \in \mathbb{R}^n$.\\
    For the Sample version, we have
    $$ \hat{\Gamma}_n = \begin{bmatrix}
        \hat{\gamma}(0) & \hat{\gamma}(1) & \hat{\gamma}(2) & \ldots & \hat{\gamma}(n-1)\\
        \hat{\gamma}(1) & \hat{\gamma}(0) & \hat{\gamma}(1) & \ldots & \hat{\gamma}(n-2)\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        \hat{\gamma}(n-1) & \hat{\gamma}(n-2) & \hat{\gamma}(n-3) & \ldots & \hat{\gamma}(0)
    \end{bmatrix}$$
    We use $n$ as a common denominator to ensure that $\hat{\Gamma}_n$ is positive semidefinite.\\
    $\Gamma_n$ is called the order-$n$ autocovariance matrix of the process.\\
    $\hat{\Gamma}_n$ is called the order-$n$ sample autocovariance
\end{remark}
\begin{theorem}[]
    A real valued fn defined on the integers is the autocovariance fn of a weakly stationary Time Series $iff$ \\
    \begin{itemize}
        \item It is even. ie $\gamma(h) = \gamma(-h)$ for all $h \in \mathbb{\mathcal{T}}$
        \item It is non-negative definite. ie for every $n \in \mathbb{N}$ and $a_1, a_2, \ldots, a_n \in \mathbb{R}$
    \end{itemize}
    IE $\sum_{i,j}^n a_i k(t_i - t_j) a_j \geq 0$ for all $n \geq 1, \mathbf{t} = (t_1, t_2, \ldots, t_n)' \in \mathbb{N}^{n}$, and $\mathbf{a} = (a_1, a_2, \ldots, a_n)' \in \mathbb{R}^n$
    \begin{proof}
        \textbf{LOOK MORE INTO THIS THEOREM}\\
        $\implies$\\
        It is straightforward to see that $\gamma_x(h)$ is even.\\
        Let $n \in \mathbb{N}$, $\mathbf{t} =
        (t_1, t_2, \ldots, t_n)' \in \mathbb{N}^{n}$, and $\mathbf{a} = (a_1, a_2, \ldots, a_n)' \in \mathbb{R}^n$\\
        $$ \sum_{i,j}^n a_i \gamma_x(t_i - t_j) a_j = \sum_{i,j}^n a_i Cov(X_{t_i}, X_{t_j}) a_j = Cov(\sum_{i=1}^{n} a_i X_{t_i}, \sum_{j=1}^{n} a_j X_{t_j}) = Var(\sum_{i=1}^{n} a_i X_{t_i}) \geq 0$$\\
        $\impliedby$\\
        Let $k(h)$ be a real valued fn defined on the integers which is even and non-negative definite.\\
        Let $n \in \mathbb{N}$, $\mathbf{t} =
        (t_1, t_2, \ldots, t_n)' \in \mathbb{N}^{n}$, and $\mathbf{a} = (a_1, a_2, \ldots, a_n)' \in \mathbb{R}^n$\\
        Define $\Gamma_n = [k(t_i -t_j)]_{i,j=1}^{n}$\\
        Then $\Gamma_n$ is a non-negative definite matrix. ie $a' \Gamma_n a \geq 0$ for all $a \in \mathbb{R}^n$.\\
        Thus by the spectral theorem, there exists a random vector $\mathbf{X} = (X_{t_1}, X_{t_2}, \ldots, X_{t_n})'$ with mean 0 and covariance matrix $\Gamma_n$. ie $E(\mathbf{X}) = 0$ and $Cov(\mathbf{X}) = \Gamma_n$.\\
        ie $Cov(X_{t_i}, X_{t_j}) = k(t_i - t_j)$ for all $1 \leq i,j \leq n$\\
        By Kolmogorov's theorem, there exists a S.P. ${X_t, t \in \mathbb{Z}}$ with autocovariance fn $k(h)$.

    \end{proof}
\end{theorem}
\begin{example}
    Suppose $k(h) = \begin{cases}
        1 & h = 0\\
        \rho & h = \pm 1\\
        0 & otherwise
    \end{cases}$\\
    When is $k$ an autocovariance fn of a weakly stationary S.P.?
    \begin{itemize}
        \item $|\rho| \leq .5$ then 
    \end{itemize}
    Remember $Z_t$ is IID(0,$\sigma^2$), $X_t = Z_t + \Theta Z_{t-1}$ with acovf $\gamma_x(h) = \begin{cases}
        (1 + \Theta^2)\sigma^2 & h = 0\\
        \Theta \sigma^2 & h = \pm 1\\
        0 & otherwise
    \end{cases}$\\
    $\rho(1) = \frac{\Theta}{1 + \Theta^2}$ then $1+ \Theta^2 \leq 2\theta$ ie $|\rho| \leq .5$\\
    \begin{itemize}
        \item If $.5 < \rho \leq 1$ then $k(h)$ is not an acovf.\\
    \end{itemize}
    Then you can find a $n$ s.t.
    $$ \sum_{i,j}^{2n} a_i a_j k(i-j) = 2n - 2(n-1)\rho < 0$$
    \fbox{\textbf{Where does this formula on the RHS come from?}}
    \begin{itemize}
        \item If $-1 \leq \rho < -.5$ then $k(h)$ is not an acovf.\\
    \end{itemize}
\end{example}
\begin{definition}[Mixing Conditions]
    Suppose $\mathcal{G}$ and $\mathcal{H}$ are two sub $\sigma$-fields on the same probability space $(\Omega, \mathcal{F}, \Prob)$. $\mathcal{G} \subset \mathcal{F}$ and $\mathcal{H} \subset \mathcal{F}$.
\end{definition}
\begin{definition}[$\alpha$-mixing:] 
    $\alpha$-mixing: $\alpha(\mathcal{G}, \mathcal{H}) = \sup_{G \in \mathcal{G}, H \in \mathcal{H}} \big|\Prob(G \cap H) - \Prob(G)\Prob(H)\big|$\\
    $X_1$ and $X_2$ are independent $\mathcal{G} = \sigma(X_1) = \sigma \langle [X_1 \leq c], c \in \mathbb{R} \rangle$ and $\mathcal{H} = \sigma(X_2)$
    \begin{itemize}
        \item $\alpha(\mathcal{G}, \mathcal{H}) = 0$ $iff$ $\mathcal{G}$ and $\mathcal{H}$ are independent
        \item $0 \leq \alpha(\mathcal{G}, \mathcal{H}) \leq .25$ for all $\mathcal{G}, \mathcal{H}$ sub $\sigma$-fields of $\mathcal{F}$
        \item $\Prob(G \cap H) - \Prob(G)\Prob(H) = \E[I_G I_H] - \E[I_G]\E[I_H] = Cov(I_G, I_H)$
    \end{itemize}
    $|Cov(c_1 I_G, c_2 I_H)| \leq c_1 c_2 \alpha(\mathcal{G}, \mathcal{H})$
\end{definition}
\begin{definition}[$\phi$-mixing:]
    $\phi$-mixing: $\phi(\mathcal{G}, \mathcal{H}) = \sup_{G \in \mathcal{G}, H \in \mathcal{H}, \Prob(G) > 0} |\Prob(H|G) - \Prob(H)|$
    \begin{itemize}
        \item $\phi(\mathcal{G}, \mathcal{H}) = 0$ $iff$ $\mathcal{G}$ and $\mathcal{H}$ are independent
        \item $0 \leq \phi(\mathcal{G}, \mathcal{H}) \leq 1$ for all $\mathcal{G}, \mathcal{H}$ sub $\sigma$-fields of $\mathcal{F}$
        \item $\alpha(\mathcal{G}, \mathcal{H}) \leq \frac{1}{2} \phi(\mathcal{G}, \mathcal{H})$
    \end{itemize}
\end{definition}
\begin{example}
    $X$ is $G$-measureable and $Y$ is $H$-measureable, $|X| \leq C_1$ and $|Y| \leq C_2$ a.s.\\
    Then $|\cov(X,Y)| \leq 4 C_1 C_2 \alpha(\mathcal{G}, \mathcal{H})$
\end{example}

\subsection{9/16/2025 Lecture 4}
\begin{remark}[Last Class Review]
    Mixing Conditions:\\
    Suppose $\mathcal{G}$ and $\mathcal{H}$ are two sub $\sigma$-fields on the same probability space $(\Omega, \mathcal{F}, \Prob)$. $\mathcal{G} \subset \mathcal{F}$ and $\mathcal{H} \subset \mathcal{F}$.
    \fbox{LOOK INTO TEXTBOOK ASSIGNMENTS}
    \begin{itemize}
        \item $\alpha$-mixing: $\alpha(\mathcal{G}, \mathcal{H}) = \sup_{G \in \mathcal{G}, H \in \mathcal{H}} \big|\Prob(G \cap H) - \Prob(G)\Prob(H)\big|$
        \item $\phi$-mixing: $\phi(\mathcal{G}, \mathcal{H}) = \sup_{G \in \mathcal{G}, H \in \mathcal{H}, \Prob(G) > 0} |\Prob(H|G) - \Prob(H)|$
    \end{itemize}
    \begin{enumerate}
        \item $\alpha(\mathcal{G}, \mathcal{H}) = 0$ $\iff$ $\mathcal{G}$ and $\mathcal{H}$ are independent
        \item $0 \leq \alpha(\mathcal{G}, \mathcal{H}) \leq .25$ for all $\mathcal{G}, \mathcal{H}$ sub $\sigma$-fields of $\mathcal{F}$, $0 \leq \phi(\mathcal{G}, \mathcal{H}) \leq 1$ for all $\mathcal{G}, \mathcal{H}$ sub $\sigma$-fields of $\mathcal{F}$
        \item $\alpha(\mathcal{G}, \mathcal{H}) \leq \frac{1}{2} \phi(\mathcal{G}, \mathcal{H})$
    \end{enumerate}
    Equal definition: $\alpha(X,Y) = \sup_{c_1, c_2 \in \mathbb{R}} | \Prob(X \leq c_1, Y \leq c_2) - \Prob(X \leq c_1)\Prob(Y \leq c_2)|$\\
    Equal definition: $\phi(X,Y) = \sup_{c_1, c_2 \in \mathbb{R}, \Prob(X \leq c_1) > 0} |\Prob(Y \leq c_2 | X \leq c_1) - \Prob(Y \leq c_2)|$\\
\end{remark}
\begin{theorem}[Ibragimov 1962]
    $\Prob(G \cap H) - \Prob(G)\Prob(H) = \cov(I_G, I_H)$\\
    $|\cov(c_1 I_G, c_2 I_H)| \leq c_1 c_2 \alpha(\mathcal{G}, \mathcal{H})$\\
    Sup. $|X| \leq C_1$ and $|Y| \leq C_2$ a.s.\\
    Then $|E(XY) - E(X)E(Y)| \leq 4 c_1 c_2 \alpha(\mathcal{G}, \mathcal{H})$
    \begin{proof}
        \begin{align*}
            E(XY) - E(X)E(Y) &= E[X(Y-E(Y))]\\
            &= E[X (E(Y|X) - E(Y))]\\
            &= E[E(XY|X) - E(Y)] \\ %help? 
            |E(XY) - E(X)E(Y)| &= | E[X(E(Y|X) - E(Y))] |\\
            &\leq c_1 E|E(Y|X) - E(Y)|\\
            \text{Define } \eta = \text{sign}(E(Y|X) - E(Y))\\
            &= c_1 E[\eta(E(Y|X) - E(Y))]\\
            \eta E(Y|X) &= E(\eta Y|X)\\
            c_1 E [E (\eta Y|X) - \eta E(Y)] &= c_1 [E(\eta Y) - E(\eta)E(Y)]\\
            E(\eta Y) - E(\eta)E(Y) &\leq E [ Y[E(\eta|Y) - E(\eta)] ]\\
            \text{Let } \xi = \text{sign}(E(\eta|Y) - E(\eta))\\
            E(\eta Y) - E(\eta)E(Y) &\leq c_2( E[\xi \eta] - E(\xi)E(\eta))\\
            E(XY) - E(X)E(Y) &\leq  c_1 c_2 (E[\xi \eta] - E(\xi)E(\eta))\\
            \eta = I_{\eta = 1} -  I_{\eta = -1} &, \xi = I_{\xi = 1} - I_{\xi = -1}\\
            \cov(\xi, \eta) &= \cov(I_{\xi = 1} - I_{\xi = -1}, I_{\eta = 1} - I_{\eta = -1})\\
            &= \cov(I_{\xi = 1}, I_{\eta = 1}) + \cov(I_{\xi = -1}, I_{\eta = -1}) \\
            &- \cov(I_{\xi = 1}, I_{\eta = -1}) - \cov(I_{\xi = -1}, I_{\eta = 1})\\
            \implies |\cov(\xi, \eta)| &\leq 4 \alpha(\mathcal{G}, \mathcal{H})\\
            |E(XY) - E(X)E(Y)| &\leq 4 c_1 c_2 \alpha(\mathcal{G}, \mathcal{H}) 
        \end{align*}
    \end{proof}
    Why are we doing this?\\
    Consider $X_1, X_2, \ldots$ IID(0, $\sigma^2$)\\
    $\frac{X_1 + X_2 + \ldots + X_n}{n} \xrightarrow{d} N(0, \sigma^2)$\\
    Now how do we get CLT?\\
    Consider $X_1, X_2, \ldots$ is a weakly stationary S.P, with $E(X_t) = 0$\\
    $\frac{X_1 + X_2 + \ldots + X_n}{n} \xrightarrow{d} N(0, \sigma^2)$\\
    We can see this is the variance $S_n = X_1 + X_2 + \ldots + X_n$\\
    $Var(S_n) = \sum_{i=1}^{n} Var(X_i) + 2 \sum_{1 \leq i < j \leq n} Cov(X_i, X_j)$\\
    $= n \gamma_x(0) + 2 \sum_{1 \leq i < j \leq n} (\gamma_x(j-i))$\\
    $= n \gamma_x(0) + 2 \sum_{h=1}^{n-1} (n-h) \gamma_x(h)$\\
    $Var(\frac{S_n}{\sqrt{n}}) = \gamma_x(0) + 2 \sum_{h=1}^{n-1} (1 - \frac{h}{n}) \gamma_x(h)$\\
    $\lim_{n \to \infty} Var(\frac{S_n}{\sqrt{n}}) = \gamma_x(0) + 2 \sum_{h=1}^{\infty} \gamma_x(h)$\\
    We want this infinite series to converge. ie $\sum_{h=1}^{\infty} |\gamma_x(h)| < \infty$.\\\\
    Consider $X_1, X_2, \ldots$ is a strictly stationary S.P.\\
    Define $\alpha_0 = \frac{1}{2}$, and $\alpha_n = \alpha(X_0, X_n)$ for $n \geq 1$\\
    Assume $E|X_0|^{p} < \infty$ for some $p > 2$ \\
    Then $|\gamma_x(k)| = |\cov(X_0, X_k)| \leq 8 ||X_0||_p^2 \alpha_k^{1 - \frac{2}{p}}$
\end{theorem}
\begin{corollary}[Only $Y$ is bounded]
    Suppose $E[X^2] < \infty$ for some $p > 1$ and $|Y| \leq C$ a.s.\\
    Then $E(XY) - E(X)E(Y) \leq 6 C ||X||_p [\alpha(X, Y)]^{1 - \frac{1}{p}}$ where $||X||_p = (E|X|^p)^{\frac{1}{p}}$
    \begin{proof}
        Through Truncation:\\
        $X_1 = X I_{|X| \leq C_1}$ and $X_2 = X - X_1$\\
        \begin{align*}
            |E(XY) - E(X)E(Y)| &\leq | E(X_1 Y) - E(X_1)E(Y)| + |E(X_2 Y) - E(X_2)E(Y)|\\
            &\leq 4 C C_1 \alpha(X, Y) + 2 C E|X_2|\\
            E|X_2| &= E|X I_{|X| > C_1}| \leq \frac{E|X|^p}{C_1^{p-1}}\\
            I_{|X| > C_1} &< \frac{|X|^p}{C_1^{p-1}}\\
            &= \frac{||X||_p^p}{C_1^{p-1}}\\
        \end{align*}
        Thus $|E(XY) - E(X)E(Y)| \leq 4 C C_1 \alpha(X, Y) + \frac{||X||_p^p}{C_1^{p-1}}$.\\
        Take $C_1 = \alpha^{-\frac{1}{p}} ||X||_p$ to get best bound.\\
        Then the corollary follows.\\
        \fbox{Look into bernstein inequality}
    \end{proof}
\end{corollary}
\begin{corollary}[No bounded (Davydov 1968)]
    Suppose $E|X|^p < \infty$  and $E|Y|^q < \infty$ for some $p, q > 1$ and $\frac{1}{p} + \frac{1}{q} < 1$ then \\
    $|E(XY) - E(X)E(Y)| \leq 8 ||X||_p ||Y||_q [\alpha(X,Y)]^{1 - \frac{1}{p} - \frac{1}{q}}$  
\end{corollary}
\textbf{Review of Hilbert Spaces}\\
\begin{definition}[Inner Product Space]
    A vector space $\mathcal{V}$ over the field $\mathbb{F}$ is called an inner product space if there exists a fn $\innerprod{\cdot}{\cdot}$
    \begin{itemize}
        \item $\innerprod{u}{v} = \overline{\innerprod{v}{u}}$ for all $u,v \in \mathcal{V}$
        \item $\innerprod{u+v}{w} = \innerprod{u}{w} + \innerprod{v}{w}$ for all $u,v,w \in \mathcal{V}$
        \item $\innerprod{cu}{v} = c \innerprod{u}{v}$ for all $u,v \in \mathcal{V}$ and $c \in \mathbb{F}$
        \item $\innerprod{u}{u} \geq 0$ for all $u \in \mathcal{V}$
        \item $\innerprod{u}{u} = 0$ iff $u = 0$ 
    \end{itemize}
    We will see that for the prob space $\innerprod{X}{Y} = E[XY]$ but this only holds a.s.
\end{definition}
\subsection{9/18/2025 Lecture 5}
\begin{definition}[Inner Product Space]
    $\mathcal{H}$ is an inner product space with inner product $\innerprod{\cdot}{\cdot}$\\
\end{definition}
\begin{example}[2.2.2]
    $L^2(\Omega, \mathcal{F}, \Prob) = \{X: \Omega \to \mathbb{R} | X \text{ is measurable and } E(X^2) < \infty\}$\\
    $\innerprod{X}{Y} = E(XY) = \int_{\Omega} X(\omega) Y(\omega) d\Prob(\omega)$\\
    $\innerprod{X}{X} = E(X^2) = 0 \implies X = 0$ a.s.\\
    Define an equivalence relation $X \sim Y$ if $X = Y$ a.s.\\
    \begin{itemize}
        \item The elements of $L^2$ are equivalence classes
        \item $||X|| = \sqrt{\innerprod{X}{X}} = \sqrt{E(X^2)}$ is a norm on $L^2$
    \end{itemize}
\end{example}
\begin{remark}
    IP Properties:\\
    \begin{itemize}
        \item $| \innerprod{x}{y} | \leq ||x|| ||y||$ (Cauchy-Schwarz Inequality)
        \item $||x+y|| \leq ||x|| + ||y||$ (Triangle Inequality)
        \item If $||x_n -x|| \to 0$ and $||y_n - y|| \to 0$ then $\innerprod{x_n}{y_n} \to \innerprod{x}{y}$ (Continuity of Inner Product)
    \end{itemize}
\end{remark}
\begin{definition}[Limit of a Sequence in Hilbert Space]
    Let $\mathcal{H}$ be a Hilbert Space and $\{x_n\}$ be a sequence in $\mathcal{H}$\\
    We say that $x_n \to x$ if $||x_n - x|| \to 0$ as $n \to \infty$.\\\\
    Suppose $\{X_n\}$ is a sequence of random variables in $L^2(\Omega, \mathcal{F}, \Prob)$ which converges to $X$. Then consider the RV $1$ (constant)\\
    Consider $\innerprod{X_n}{1} \to \innerprod{X}{1}$\\
    ie $E(X_n) \to E(X)$\\\\
    $X_n \to X$ \\
    $\innerprod{X_n}{X_n} \to \innerprod{X}{X}$
    ie $E(X_n^2) \to E(X^2)$\\\\
    $X_n \to X$, $Y_n \to Y$\\
    $\innerprod{X_n}{Y_n} \to \innerprod{X}{Y}$\\
    ie $E(X_n Y_n) \to E(XY)$
\end{definition}
\begin{definition}[Cauchy Sequence]
    A sequence of elemnts $\{x_n\}$ in an inner product space $\mathcal{H}$ is called a Cauchy sequence if for every $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that $||x_n - x_m|| < \epsilon$ for all $n,m \geq N$.
\end{definition}
\begin{definition}[Hilbert Space]
    An inner product space $\mathcal{H}$ is called a Hilbert Space if every Cauchy sequence in $\mathcal{H}$ converges to an element in $\mathcal{H}$.
\end{definition}
\begin{example}
    Consider $\mathcal{M}(\Omega, \mathcal{F}, \Prob) = \{X: |X| \leq C, C > 0\}$\\
    $\innerprod{X}{Y} = E(XY)$\\
    $X \sim N(0,1)$\\
    $X_n = X I_{|X| \leq n}$\\
    $E|X-X_n|^2 = E[X^2 I_{|X| > n}] \to 0$ by DCT (Dominated Convergence Theorem)\\
    So $X_n \to X$ in $L^2$ but $X \notin \mathcal{M}$\\
    Thus $\mathcal{M}$ is not a Hilbert Space.
\end{example}
\begin{definition}[Complex Random Variable]
    A complex random variable is a fn $Z: \Omega \to \mathbb{C}$ such that $Z = X + iY$ where $X, Y$ are real random variables.
\end{definition}
\begin{definition}[Closed Subspace]
    A linear subspace of a Hilbert Space $\mathcal{H}$ is called a closed subspace if $\mathcal{M}$ contains its limit points. ie if $\{x_n\} \subset \mathcal{M}$ and $x_n \to x$ in $\mathcal{H}$ then $x \in \mathcal{M}$.
\end{definition}
\begin{proposition}[2.3.1] \fbox{Review the definition}
    If $\mathcal{M}$ is a closed subset of a H.S $\mathcal{H}$ then the orthogonal compliment $\mathcal{M}^{\perp} = \{x \in \mathcal{H}: x \perp y, \forall y \in \mathcal{M} \}$ closed linear subspace of $\mathcal{H}$.
\end{proposition}
\begin{theorem}[2.3.1 Projection Theorm]
    If $\mathcal{M}$ is a closed linear subspace of a H.S $\mathcal{H}$ and $x \in \mathcal{H}$ then \\
    (i) there is a unique element $\hat{x} \in \mathcal{M}$ such that $||x - \hat{x}|| = \inf_{y \in \mathcal{M}} ||x - y||$\\
    (ii) $\hat{x} \in \mathcal{M}$ and $||x - \hat{x}|| = \inf_{y \in \mathcal{M}} ||x - y||$ iff 
    $\hat{x} \in \mathcal{M}$ and $x - \hat{x} \in \mathcal{M}^{\perp}$\\
\end{theorem}
\begin{definition}[2.4.1 Closed Span]
    The closed span $\overline{sp} \{X_t, t \in \mathcal{T}\}$ of any subset $\{X_t, t \in \mathcal{T}\}$ of a H.S $\mathcal{H}$ is the smallest closed linear subspace of $\mathcal{H}$ containing $\{X_t, t \in \mathcal{T}\}$. 
\end{definition}
\begin{definition}[Orthonormal Set]
    A set $\{e_t: t \in \mathcal{T} \}$ of element of an IP space is said to be orthonormal if $\innerprod{e_s}{e_t} = \begin{cases}
        1 & s = t\\
        0 & s \neq t
    \end{cases}$ for all $s,t \in \mathcal{T}$
\end{definition}
\begin{definition}[Complete Orthonormal Set]
    An orthonormal set $\{e_t: t \in \mathcal{T} \}$ in a H.S $\mathcal{H}$ is said to be complete if $\overline{sp}\{e_t, t \in \mathcal{T}\} = \mathcal{H}$
\end{definition}
\begin{definition}[Seperability]
    The HS is separable if it has a finite or countable infinite complete orthonormal set.
\end{definition}
\begin{example}[Separable HS]
    1. $\mathbb{R}^d$ \\
    2. $L^2(\Omega, \mathcal{F}, \Prob)$
\end{example}
\begin{theorem}[2.4.2]
    If $\mathcal{H}$ is a separable H.S and $\mathcal{H} = \overline{sp}\{e_t: t \in \mathcal{T}\}$ where $\{e_t: t \in \mathcal{T}\}$ is an orthonormal set then 
    \begin{itemize}
        \item The set of all finite linear combinations of $\{e_t: t \in \mathcal{T}\}$ is dense in $\mathcal{H}$. ie for every $x \in \mathcal{H}$ and $\epsilon > 0$ there exists $y = \sum_{j=1}^{n} a_j e_{t_j}$ such that $||x - y|| < \epsilon$
        \item $ x  = \sum_{i = 1}^{\infty} \innerprod{x}{e_i} e_i$ for each $x \in \mathcal{H}$ ie $||x - \sum_{i=1}^{n} \innerprod{x}{e_i} e_i|| \to 0$ as $n \to \infty$
        \item $||x||^2 = \sum_{i=1}^{\infty} |\innerprod{x}{e_i}|^2$ for each $x \in \mathcal{H}$ (Parseval's Identity)
        \item $\innerprod{x}{y} = \sum_{i=1}^{\infty} \innerprod{x}{e_i} \innerprod{y}{e_i}$ for all $x,y \in \mathcal{H}$
        \item $x = 0 \iff \innerprod{x}{e_i} = 0$ for all $i \geq 1$
    \end{itemize}
\end{theorem}
\subsection{9/23/2025 Lecture 6}
\begin{definition}[ARMA models: ARMA$(p,q)$]
    Let $\{Z_t\} \sim WN(0, \sigma^2)$. The process $\{X_t, t \in \mathbb{Z}\}$ is said to be an $ARMA(p,q)$ process if
    \begin{itemize}
        \item $\{X_t\}$ is stationary for all $t \in \mathbb{Z}$
        \item $X_t - \phi_1 X_{t-1} - \ldots - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ for all $t \in \mathbb{Z}$ where $\phi_1, \ldots, \phi_p, \theta_1, \ldots, \theta_q$ are real constants with $\phi_p, \theta_q \neq 0$.
    \end{itemize}
\end{definition}
\begin{remark}
    There are a few special cases of the $ARMA(p,q)$ model:
    \begin{itemize}
        \item When $q = 0$ we can write the model as $X_t - \phi_1 X_{t-1} - \ldots - \phi_p X_{t-p} = Z_t$ and call it an $AR(p)$ model.
        \item When $p = 0$ we can write the model as $X_t = Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ and call it a $MA(q)$ model.
        \item When $p = 0$ and $q = 0$ we have $X_t = Z_t$ and call it a white noise model.
        \item $\{X_t\}$ is defined relative to the white noise process $\{Z_t\}$. 
        \item Staitionarity is a critial requirement for the $ARMA(p,q)$ model.
    \end{itemize}
    \begin{itemize}
        \item AR polynomial: $\phi(z) = 1 - \phi_1 z - \ldots - \phi_p z^p$
        \item MA polynomial: $\theta(z) = 1 + \theta_1 z + \ldots + \theta_q z^q$
        \item Backshift operator: $B X_t = X_{t-1}$, $B^2 X_t = X_{t-2}$, $\ldots$, $B^k X_t = X_{t-k}$
        \item AR$(p)$ model: $\phi(B) X_t = Z_t$
        \item MA$(q)$ model: $X_t = \theta(B) Z_t$
        \item ARMA$(p,q)$ model: $\phi(B) X_t = \theta(B) Z_t$
        \item More general model with a mean: $\{X_t + \mu: t \in \mathbb{Z}\}$
        \item Can also be characterized by $X_t = \phi_0 + \phi_1 X_{t-1} + \ldots + \phi_p X_{t-p} + Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ where $\phi_0 = \mu (1 - \phi_1 - \ldots - \phi_p)$
    \end{itemize}
\end{remark}
\begin{example}[Staitionary solution to AR$(1)$]
    \begin{align*}
        X_t &= \phi X_{t-1} + Z_t \\
        &= Z_t + \phi (Z_{t-1} + \phi X_{t-2}) = Z_t + \phi Z_{t-1} + \phi^2 X_{t-2}\\
        &= Z_t + \phi Z_{t-1} + \phi^2 (Z_{t-2} + \phi X_{t-3}) = Z_t + \phi Z_{t-1} + \phi^2 Z_{t-2} + \phi^3 X_{t-3}\\
        &\vdots \\
        &= Z_t + \phi Z_{t-1} + \phi^2 Z_{t-2} + \ldots + \phi^k Z_{t-k} + \phi^{k+1} X_{t-(k+1)}
    \end{align*}
    If $|\phi| < 1$ then $\phi^{k+1} X_{t-(k+1)} \to 0$ as $k \to \infty$\\
    Thus the stationary solution is $X_t = \sum_{j=0}^{\infty} \phi^j Z_{t-j}$\\
    If $|\phi| \geq 1$ then there is no stationary solution since we can see that 
    $X_{t+1} = \phi X_t + Z_{t+1} \iff X_t = -\frac{1}{\phi} Z_{t+1} + \frac{1}{\phi} X_{t+1}$
    \begin{align*}
        X_t &= \phi^{-1} X_{t+1} - \phi^{-1} Z_{t+1} \\
        &= \phi^{-1} (\phi^{-1} X_{t+2} - \phi^{-1} Z_{t+2}) - \phi^{-1} Z_{t+1}\\
        &= \phi^{-2} X_{t+2} - \phi^{-1} Z_{t+1} - \phi^{-2} Z_{t+2}\\
        &\vdots \\
        &= \phi^{-k} X_{t+k} - \sum_{j=1}^{k} \phi^{-j} Z_{t+j} \\
        &= - \sum_{j=1}^{\infty} \phi^{-j} Z_{t+j}
    \end{align*}
    \fbox{We will see later that why this is the unique stationary solution when $|\phi| < 1$}
\end{example}
\begin{remark}
    Uniqueness of stationary solution to AR$(1)$:\\
    \begin{itemize}
        \item If $X_t = \phi X_{t-1} + Z_t$, where $|\phi| > 1$ then we can rewrite this as $X_t = \phi^{\star} X_{t-1} + Z_t^{\star}$ with $\phi^{\star} < 1$ and $Z_t^{\star} \sim WN(0, \sigma^2)$
        \fbox{Homework problem}
    \end{itemize}
\end{remark}
\begin{definition}[3.1.3: Causality]
    An ARMA$(p,q)$ process $\phi(B)X_t = \theta(B) Z_t$ is said to be causal if ther exists a sequence of constants $\{\psi_j\}$ such that $\sum_{j=0}^{\infty} |\psi_j| < \infty$ and $X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j}$ for all $t \in \mathbb{Z}$.
\end{definition}
\begin{proposition}[3.1.1]
    If $\{X_t, t \in \mathbb{Z}\}$ is a sequence of rv st. $\sup_t E|X_t| < \infty$ and if $\{\psi_j\}_{j \geq 0}$ is a sequence of numbers s.t $\sum_{j=0}^{\infty} |\psi_j| < \infty$ then the series $\psi(B) X_t = \left( \sum_{j=0}^{\infty} \psi_j B^j \right) X_t = \sum_{j=0}^{\infty} \psi_j X_{t-j}$ converges absolutely w.p 1\\
    If in addition $\sup_t E(X_t^2) < \infty$ then the series converges in $L^2$ to the same limit.
    \begin{proof}
        \begin{itemize}
            \item Consider $\sum_{j=0}^{\infty} |\psi_j| |X_{t-j}|$, which always exists (may be infinite)
            \item Monotone Convergence Theorem implies $E \left( \sum_{j=0}^{\infty} |\psi_j| |X_{t-j}| \right) = \sum_{j=0}^{\infty} |\psi_j| E|X_{t-j}| \leq \left( \sup_t E|X_t| \right) \sum_{j=0}^{\infty} |\psi_j| < \infty \implies \sum_{j=0}^{\infty} |\psi_j| |X_{t-j}| < \infty$ w.p 1
            \item $\implies \sum_{j=0}^{\infty} \psi_j X_{t-j}$ converges absolutely w.p 1, call the limit $W_t$.
            \item Verify $\sum_{j=0}^{n} \psi_j X_{t-j}$ is a Cauchy sequence in $L^2$: We do this by showing $|| \sum_{j=n}^{m} \psi_j X_{t-j} ||_2 \to 0$ as $n,m \to \infty$.\\
            \item So it converges in $L^2$ to some limit $S_t$.
            \item $E(S_t - W_t)^2 = E[ \lim \inf_n (S - \sum_{j=0}^{n} \psi_j X_{t-j})^2]$ by Fatou's Lemma\\
            $\leq \lim \inf_n E(S - \sum_{j=0}^{n} \psi_j X_{t-j})^2 = 0$\\
            $\implies S_t = W_t$ a.s. since the second moment is 0.
        \end{itemize}
    \end{proof}
\end{proposition}
\subsection{9/25/2025 Lecture 7}
\begin{remark}[Review]
    Review of last week:
    \begin{itemize}
        \item ARMA$(p,q)$ process: $\phi(B) X_t = \theta(B) Z_t$ where $\{Z_t\} \sim WN(0, \sigma^2)$
        \item MA$(q)$ process: $X_t = \theta(B) Z_t$
    \end{itemize}
\end{remark}
\begin{proposition}[3.1.2]
    If $\{X_t\}$ is a staitionary process with autocovariance function $\gamma_x(\cdot)$ and if $\{\psi_j\}_{j \geq 0}$, $\sum_{j=0} |\psi_j| < \infty$, define $Y_t = \sum_{j=0}^{\infty} \psi_j X_{t-j}$ (converges absolutely, w.p 1).\\
    Then $Y_t$ is also staitionary with autocovariance function $\gamma_y(h) = \sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} \psi_j \psi_k \gamma_x(h + j - k)$ where $\psi_j = 0$ for $j < 0$.
    \begin{proof}
        We need to show that $E(Y_t)$ is constant and $\cov(Y_{t+h}, Y_t)$ depends only on $h$.\\
        \begin{align*}
            E(Y_t) &= E \left( \sum_{j=0}^{\infty} \psi_j X_{t-j} \right) = \sum_{j=0}^{\infty} \psi_j E(X_{t-j}) = \mu_x \sum_{j=0}^{\infty} \psi_j \text{ (constant)}\\
            \cov(Y_{t+h}, Y_t) &= E[(Y_{t+h} - E(Y_{t+h}))(Y_t - E(Y_t))]\\
            &= E \left[ \left( \sum_{j=0}^{\infty} \psi_j (X_{t+h-j} - \mu_x) \right) \left( \sum_{k=0}^{\infty} \psi_k (X_{t-k} - \mu_x) \right) \right]\\
            &= E \left[ \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \psi_j \psi_k (X_{t+h-j} - \mu_x)(X_{t-k} - \mu_x) \right]\\
            &= \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \psi_j \psi_k E[(X_{t+h-j} - \mu_x)(X_{t-k} - \mu_x)]\\
            &= \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \psi_j \psi_k \gamma_x(h + j - k)\\
            &= \sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} \psi_j \psi_k \gamma_x(h + j - k) \text{ where } \psi_j = 0 \text{ for } j < 0
        \end{align*}
    \end{proof}
\end{proposition}
\begin{remark}
    Let $\alpha(B) = \sum_{j=0}^{\infty} \alpha_j B^j$ and $\beta(B) = \sum_{j=0}^{\infty} \beta_j B^j$ $\sum_{j=0}^{\infty} |\alpha_j| < \infty$ $\sum_{j=0}^{\infty} |\beta_j| < \infty$.\\
    Then the product $\psi(B) = \alpha(B) \beta(B) = \sum_{j=0}^{\infty} \psi_j B^j$ then $\sum_{j=0}^{\infty} |\psi_j| < \infty$ 
\end{remark}
\begin{theorem}[3.1.1.a]
    If $\phi(z)$ and $\theta(z)$ have no common zeros, if $phi(z) \neq 0$ for $|z| = 1$ and if $\{Z_t\} \sim WN(0, \sigma^2)$ then exists a unique stationary solution given by 
    $$ X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j} \text{ where } \psi(z) = \frac{\theta(z)}{\phi(z)} = \sum_{j=0}^{\infty} \psi_j z^j$$
    and $\sum_{j=0}^{\infty} |\psi_j| < \infty$. so that $X_t$ is well-defined and causal.
    \begin{proof}
        (i) Find Solution\\
        If $\phi(z) \neq 0$ for $|z| = 1$ then $\exists \epsilon > 0$ such that 
        \begin{align*}
            \frac{1}{\phi(z)} &:= \sum_{j=0}^{\infty} \zeta_j z^j =: \zeta(z), |z| \leq 1 + \epsilon\\
            &\implies |\zeta_j| \leq (1 + \epsilon/2)^{-j} \text{ for some } K > 0
        \end{align*}
        Consider $\frac{1}{1-z} = \sum_{j=0}^{\infty} z^j$ for $|z| < 1$\\
        Consider $\frac{1}{1 - 0.5z} = \sum_{j=0}^{\infty} (0.5z)^j$ for $|z| < 2$\\
        $\phi(z) = \prod_{j=1}^{p} (1 - w_j z)$, ie each of the roots are $\frac{1}{w_j}$.\\
        Then $\frac{1}{\phi(z)} = \prod_{j=1}^{p} \frac{1}{1 - w_j z}$\\
        $\implies \frac{1}{\phi(z)} = \sum_{j=0}^{\infty} \zeta_j z^j$ for $|z| < \min_{1 \leq j \leq p} |w_j|^{-1}$\\
        We know that $\forall j, |w_j| < 1$ and then if we take $\epsilon = \min_{1 \leq j \leq p} |w_j|^{-1} - 1 > 0$ then we are done.\\
        (ii) Find Stationary Solution\\
        Define $X_t = \frac{\theta(B)}{\phi(B)} Z_t$ which is staitionary\\
        $\phi(B) X_t = \theta(B) Z_t$\\
        (iii) Uniqueness of Stationary Solution\\
        Suppose $\{W_t\}$ is another stationary solution to $\phi(B) W_t = \theta(B) Z_t$\\
        \begin{align*}
            \phi(B) W_t &= \theta(B) Z_t \\
            \zeta(B [ \phi(B) W_t]) &= \zeta(B [ \theta(B) Z_t])\\
            \implies W_t &= \zeta(B) [\theta(B) Z_t] = \frac{\theta(B)}{\phi(B)} Z_t = X_t
        \end{align*}
    \end{proof}
\end{theorem}
\begin{theorem}[3.1.1.b]
    Assume $\phi(z)$ and $\theta(z)$ have no common zeros. If there exists a stationary solution which is also causal then $\phi(z) \neq 0$ for $|z| \leq 1$.
\end{theorem}
\subsection{9/30/2025 Lecture 8}
\begin{remark}[Review]
    Prior class review:
    \begin{itemize}
        \item ARMA$(p,q)$ process: $\phi(B) X_t = \theta(B) Z_t$ where $\{Z_t\} \sim WN(0, \sigma^2)$
    \end{itemize}
    $\phi(z)$ and $\theta(z)$ have no common zeros.\\
\end{remark}
\begin{theorem}[3.1.1.a \& .b]
    (a) If $\phi(z) \neq 0$ for all $|z| \leq 1$ then there exists a unique stationary solution given by
    $$ X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j} \text{ where } \psi(z) = \frac{\theta(z)}{\phi(z)} = \sum_{j=0}^{\infty} \psi_j z^j$$
    and they satisfy $\sum_{j=0}^{\infty} |\psi_j| < \infty$. \\
    (b) If there exists a stationary solution which is also causal then $\phi(z) \neq 0$ for all $|z| \leq 1$.
\end{theorem}
\begin{remark}
    Not proving
    \begin{itemize}
        \item If $\phi(z) \neq 0$ for all $|z| = 1$ then there a unique stationary solution. 
        \item If $\phi(z) = 0$ for some $|z| = 1$ then there is no stationary solution.
        \item If $\phi(z) \neq 0$ for all $|z| = 1$ and $\{X_t\}$ is the unique staitionary solution then one can find $\hat{phi}(z)$ and $WN\{Z_t^*\}$ st $\hat{\phi}(z) X_t = \phi(B) Z_t^*$ and $\hat{\phi}(z) \neq 0$ for all $|z| \leq 1$. 
        \item Only Focus on Causal and Invertable ARMA models
    \end{itemize}
\end{remark}
\begin{definition}[3.1.4]
    Suppose $\{X_t\}$ is a staitionary solution of $\phi(B) X_t = \theta(B) Z_t$, it is said to be invertible if $\exists{\pi_j}$ such that $\sum_{j=0}^{\infty} |\pi_j| < \infty$  and $Z_t = \sum_{j=0}^{\infty} \pi_j X_{t-j}$ for all $t \in \mathbb{Z}$.
\end{definition}
\begin{theorem}[3.1.2]
    Suppose $X_t$ is the unique stationary solution of $\phi(B) X_t = \theta(B) Z_t$, then it is invertible iff $\theta(z) \neq 0$ for all $|z| \leq 1$. \\
    When the condition holds $\{\pi_j\}$ are determined by $\pi(z) = \frac{\phi(z)}{\theta(z)} = \sum_{j=0}^{\infty} \pi_j z^j$.
\end{theorem}
\begin{remark}
    IF the definition of invertability is relaxted to:
    $$ Z_t \in \overline{sp} \{X_{t}, X_{t-1}, \ldots \}$$
    then the condition relaxted to $\theta(z) \neq 0$ for all $|z| < 1$
\end{remark}
\begin{definition}[3.2.1]
    Suppose $\{Z_t\} \sim WN(0, \sigma^2)$, we say $\{X_t\}$ is an infinite order moving average denoted by $MA(\infty)$ if
    $$\exists \{\psi_j\} \text{ such that } \sum_{j=0}^{\infty} |\psi_j| < \infty \text{ and } X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j}$$
    May relax condition to $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ then take $X_t$ as the $L^2$ limit.\\
    Sometimes MA$(\infty)$ is called the linear process.\\
    This is related to the Wold Decomposition Theorem.
\end{definition}
\begin{proposition}[3.2.1]
    If $\{X_t\}$ is a zero-mean stationary process with autocovariance function $\gamma_x(\cdot)$ such that $\gamma_x(h) = 0$ for $|h| > q$ and $\gamma_x(q) \neq 0$ then $\{X_t\}$ is an $MA(q)$ process.\\
    IE: $\exists WN \{Z_t\}$ s.t. $X_t = Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ where $\theta_q \neq 0$.
    \begin{proof}
        \begin{itemize}
            \item  Find the WN $\{Z_t\}$
            \item Show that $X_t = Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ for some $\theta_1, \ldots, \theta_q$ with $\theta_q \neq 0$
        \end{itemize}
    \end{proof}
\end{proposition}
\begin{definition}[Linear Predictor]
    Suppose $Y \in \mathbb{R}$, $E[Y] = 0$, $\mathbf{X} \in \mathbb{R}^d$, $E[\mathbf{X}] = \mathbf{0}$.
    $$ \cov(\begin{bmatrix}
    Y \\
    \mathbf{X}
    \end{bmatrix}) = \begin{bmatrix}
    \sigma_Y^2 & \mathbf{\sigma_{YX}'} \\
    \mathbf{\sigma_{YX}} & \Sigma_X
    \end{bmatrix} $$
    A linear predictor takes the form $C^T X$ where $C \in \mathbb{R}^d$.\\
    The best linear predictor (BLP) of $Y$ based on $\mathbf{X}$ is the linear predictor $\hat{Y} = C^T \mathbf{X}$ that minimizes the mean squared error $\min_{C \in \mathbb{R}^d} E[(Y - C^T \mathbf{X})^2]$.
    \begin{align*}
        E[(Y - C^T \mathbf{X})^2] &= E[Y^2] - 2C^T E[Y \mathbf{X}] + C^T E[\mathbf{X} \mathbf{X}^T] C \\
        &= \sigma_Y^2 - 2C^T \mathbf{\sigma_{YX}} + C^T \Sigma_X C
    \end{align*}
    The best solution is given taking the partial derivative and setting it to 0:
    \begin{align*}
        \frac{\partial}{\partial C} E[(Y - C^T \mathbf{X})^2] &= -2 \mathbf{\sigma_{YX}} + 2 \Sigma_X C = 0\\
        \implies \hat{C} &= \Sigma_X^{-1} \mathbf{\sigma_{YX}}\\
        \implies \hat{Y} &= \hat{C}^T \mathbf{X} = \mathbf{\sigma_{YX}'} \Sigma_X^{-1} \mathbf{X}
    \end{align*}
    $$E[(Y - \hat{Y})^2] = \sigma_Y^2 - \mathbf{\sigma_{YX}'} \Sigma_X^{-1} \mathbf{\sigma_{YX}}$$
\end{definition}
\begin{remark}
    $\{X_t\}$ is a mean-zero stationary process.\\
    Want to predict $X_{k+1}$ based on $\{X_1, \ldots, X_k\}$.\\
    $$ \min_{\phi_1, \ldots, \phi_k} E[(X_{k+1} - \hat{X}_{k+1})^2] $$
    Where $\hat{X}_{k+1} = \sum_{j=1}^{k} \phi_j X_{k+1-j}$\\
    $$Gamma_{k+1} =\cov(\begin{bmatrix}
    X_{k+1} \\
    X_k \\
    \vdots \\
    X_1
    \end{bmatrix}) = \begin{bmatrix}
        \gamma(0) & \mathbf{\gamma(k)}' \\
        \mathbf{\gamma(k)} & \Gamma_k
    \end{bmatrix}$$ 
    Where $\mathbf{gamma(k)} = [\gamma(1), \ldots, \gamma(k)]'$ and $\Gamma_k = [\gamma(i-j)]_{i,j=1}^{k}$\\
    $$\begin{bmatrix}
    \hat{\phi}_1 \\
    \hat{\phi}_2 \\
    \vdots \\
    \hat{\phi}_k
    \end{bmatrix} = \Gamma_k^{-1} \mathbf{\gamma(k)}$$
\end{remark}
\subsection{10/2/2025 Lecture 9}
\begin{proposition}[3.2.1]
    IF $\{X_t\}$ is a zero-mean stationary process with autocovariance function $\gamma_x(\cdot)$ such that $\gamma_x(h) = 0$ for $|h| > q$ and $\gamma_x(q) \neq 0$ then $\{X_t\}$ is an $MA(q)$ process.\\
    \begin{proof}
        Need to show 
        \begin{itemize}
            \item Find $WN \{Z_t\}$
            \item Show that $X_t = Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}$ for some $\theta_1, \ldots, \theta_q$ with $\theta_q \neq 0$
        \end{itemize}
        Linear prediction problem: Predict $X_{k+1}$ using $X_1, \ldots, X_k$
        \begin{align*}
            \underline{\hat{\phi_{k}}} &= \arg \min_{\phi_1, \ldots, \phi_k} E[(X_{k+1} - \hat{X}_{k+1})^2] \\
            \hat{X}_{k+1} &= \sum_{j=1}^{k} \phi_j X_{k+1-j}
        \end{align*}
        $ \underline{\hat{\phi_k}} = \Gamma_k^{-1} \underline{\gamma(k)}$ where $\underline{\gamma(k)} = [\gamma(1), \ldots, \gamma(k)]'$\\
        $E[(X_{k+1} - \hat{\phi_k}' \underline{X_k})^2] = \gamma(0) - \underline{\hat{\gamma(k)}}' \Gamma_k^{-1} \underline{\gamma(k)} = \nu_k$\\
        Note that if we want 
        \begin{align*}
            X_t &= Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q} \\
            Z_t &= \sum_{j=0}^{\infty} \psi_j X_{t-j} \\
            Z_{t-1}, \ldots, Z_{t-q} &\in \overline{sp} \{X_{t-1}, X_{t-2}, \ldots \}
        \end{align*}
        \textbf{Proof:}\\
        Define $\mathcal{M}_{t} = \overline{sp} \{X_{t}, X_{t-1}, \ldots\}$ and $Z_t = X_t - \mathcal{P}_{\mathcal{M}_{t-1}} (X_t)$ Notice that $Z_t \in \mathcal{M}_t$ and $Z_t \perp \mathcal{M}_{t-1}$\\
        \begin{itemize}
            \item $E[Z_s Z_t] = 0$ if $s \neq t$
            $s > t$ . \\
        \begin{align*}
            Z_t &\in \mathcal{M}_t \subset \mathcal{M}_{s-1} \\
            Z_s &:= X_s - \mathcal{P}_{\mathcal{M}_{s-1}}(X_s)\\
            Z_s \perp \mathcal{M}_{s-1} & \implies Z_s \perp Z_t \\
            X_t &= Z_t + \phi_1 Z_{t-1} + \ldots + \phi_q Z_{t-q} \text{ for some } \phi_1, \ldots, \phi_q\\
            Z_t &= \sum_{j=0}^{\infty} \psi_j X_{t-j} \text{ for some } \psi_j\\
            Z_{t-1}, \ldots, Z_{t-q} &\in \overline{sp} \{X_{t-1}, X_{t-2}, \ldots\}
        \end{align*}
        and $\mathcal{P}_{\overline{sp} \{X_{t-1}, X_{t-2}, \ldots\}} (X_t) \to \mathcal{P}_{\mathcal{M}_{t}} X_t$ in $L^2$ as $k \to \infty$.
        \item $||Z_t|| = \lim_{n \to \infty} ||X_t - \mathcal{P}_{\overline{sp} \{X_{t-1}, \ldots, X_{t-n}\}} (X_t)|| $\\
        $ = \lim_{n \to \infty} E[(X_{t-1} - \mathcal{P}_{\overline{sp} \{X_{t-2}, \ldots, X_{t-n-1}\}} (X_t))] = ||Z_{t-1}||$\\
        Denote $\sigma^2 = ||Z_t||^2$ then $\{Z_t\} \sim WN(0, \sigma^2)$
        \item $\mathcal{M}_{t-1} = \overline{sp} \{X_{t-1}, X_{t-2}, \ldots\} = \overline{sp} \{Z_{t-1},\ldots Z_{t-q}, \ldots X_{t-q-1}, X_{t-q-2}, \ldots\}$\\
        Also $\overline{sp} \{Z_{t-1}, \ldots, Z_{t-q}\} \perp \overline{sp} \{X_{t-q-1}, X_{t-q-2}, \ldots\}$\\
        So $\mathcal{M}_{t-1} = \overline{sp} \{Z_{t-1}, \ldots, Z_{t-q}\} \oplus \mathcal{M}_{t-q-1}$\\
        \begin{align*}
            \mathcal{P}_{\mathcal{M}_{t-1}} (X_t) &= \mathcal{P}_{\overline{sp} \{Z_{t-1}, \ldots, Z_{t-q}\}} (X_t) + \mathcal{P}_{\mathcal{M}_{t-q-1}} (X_t)\\
            &= \frac{\innerprod{X_t}{Z_{t-1}}}{\innerprod{Z_{t-1}}{Z_{t-1}}} Z_{t-1} + \ldots + \frac{\innerprod{X_t}{Z_{t-q}}}{\innerprod{Z_{t-q}}{Z_{t-q}}} Z_{t-q} + \mathcal{P}_{\mathcal{M}_{t-q-1}} (X_t)\\
            \text{Note that } \gamma_x(h) = 0 \text{ for } |h| > q &\implies \mathcal{P}_{\mathcal{M}_{t-q-1}} (X_t) = 0\\
            \implies X_t &= Z_t + \sum_{j=1}^{q} \frac{\innerprod{X_t}{Z_{t-j}}}{\sigma^2} Z_{t-j}
        \end{align*}
        \end{itemize}
    \end{proof}
\end{proposition}
\begin{proposition}[5.2.1 Durbin Levinson Algorithm]
    Suppose $\{X_t\}$ is a zero-mean stationary process with autocovariance function $\gamma(\cdot)$. Such that $\gamma(0) > 0$ and $\gamma(h) \to 0$ as $h \to \infty$.\\
    Then $\phi_{11} = \rho(1)$ and $\nu_1 = \gamma(0) (1 - \rho(1)^2)$\\
    $ \phi_{nn} = [\gamma(n) - \sum_{j=1}^{n-1} \phi_{n-1,j} \gamma(n-j)] \nu_{n-1}^{-1}$ for $n \geq 2$\\
    \begin{align*}
        \begin{bmatrix}
        \phi_{n1} \\
        \phi_{n2} \\
        \vdots \\
        \phi_{n,n-1} 
        \end{bmatrix} = \begin{bmatrix}
        \phi_{n-1,1} \\
        \phi_{n-1,2} \\
        \vdots \\
        \phi_{n-1,n-1}
        \end{bmatrix} - \phi_{nn} \begin{bmatrix} 
        \phi_{n-1,n-1} \\
        \phi_{n-1,n-2} \\
        \vdots \\
        \phi_{n-1,1}
        \end{bmatrix} \text{ for } n \geq 2
    \end{align*}
    $\nu_n = \nu_{n-1} (1 - \phi_{nn}^2)$\\
    Given $\phi_{n-1}$ and $\nu_{n-1}$ How do we get $\phi_n$ and $\nu_n$?\\
    The complexity is $O(n^2)$ instead of $O(n^3)$.
    \begin{proof}
        (1) $\phi_{11} = \rho(1)$ and $\nu_1 = \gamma(0) [1 - \rho(1)^2]$\\
        Suppose we have $\underline{\phi_{n-1}}$ and $\nu_{n-1}$, we want to find $\underline{\phi_n}$ and $\nu_n$.\\
        Want: $\mathcal{P}_{\overline{sp} \{X_1, \ldots, X_{n}\}} (X_{n+1})$\\ 
        Let $w_1 = X_1 - \mathcal{P}_{\overline{sp} \{X_2, \ldots, X_n\}} (X_1)$ then $\begin{cases}
            w_1 \perp \overline{sp} \{X_{n}, \ldots, X_{n-2}\}\\
            \overline{sp} \{X_1, \ldots, X_n\} = \overline{sp} \{w_1\} \oplus \overline{sp} \{X_2, \ldots, X_n\}
        \end{cases}$\\
        $\mathcal{P}_{\spn\{X_n \ldots X_2\}}(X_{n+1}) = \underline{\phi_{n-1}}' \begin{bmatrix}
        X_n \\
        X_{n-1} \\
        \vdots \\
        X_2
        \end{bmatrix}$ Then want $\frac{\innerprod{X_{n+1}}{w_1}}{\innerprod{w_1}{w_1}} w_1$\\
        $\mathcal{P}_{\overline{sp} \{X_n, \ldots, X_{n-2}\}} (X_1) = \underline{\phi_{n-1}}' \begin{bmatrix}
            X_1 \\
            X_2 \\
            \vdots \\
            X_{n}
        \end{bmatrix}$\\
        $ w_1 = X_1 - \underline{\phi_{n-1}}' \begin{bmatrix}
            X_1 \\
            X_2 \\
            \vdots \\
            X_{n}
        \end{bmatrix} \quad || w_1||^2 = \nu_{n-1} \quad \innerprod{X_{n+1}}{w_1} = \gamma(n) - \underline{\phi_{n-1}}' \begin{bmatrix}
            \gamma(n-1) \\
            \gamma(n-2) \\
            \vdots \\
            \gamma(1)
        \end{bmatrix}$\\
        $\mathcal{P}_{\spn_{w_1}} (X_{n+1}) = \frac{\innerprod{X_{n+1}}{w_1}}{\innerprod{w_1}{w_1}} w_1 = \phi_{nn} w_1$ 
        $\underline{\phi_n}' \begin{bmatrix}
            X_n \\
            X_{n-1} \\
            \vdots \\
            X_1
        \end{bmatrix} - \phi_{nn} \begin{bmatrix}
            \phi_{n-1,n-1} \\
            \phi_{n-1,n-2} \\
            \vdots \\
            \phi_{n-1,1}
        \end{bmatrix}$
        $\nu_n = \nu_{n-1} - \phi_{nn}^2 \nu_{n-1} = (1 - \phi_{nn}^2) \nu_{n-1}$
    \end{proof}
    \fbox{PLEASE REVIEW THIS WHAT THE HELL IS THIS}
\end{proposition}
\subsection{10/7/2025 Lecture 10}
\begin{remark}
    Review of last class:
    \begin{itemize}
        \item Linear Predictor: Suppose $Y \in \mathbb{R}$, $E[Y] = 0$, $\mathbf{X} \in \mathbb{R}^d$, $E[\mathbf{X}] = \mathbf{0}$.
        $$ \cov(\begin{bmatrix} Y \\ \mathbf{X} \end{bmatrix}) = \begin{bmatrix} \sigma_Y^2 & \Sigma_{YX} \\ \Sigma_{XY} & \Sigma_X \end{bmatrix} $$
    \end{itemize}
    The BLP is given by $\hat{X} = \phi_n' X_n$ where $\phi_n = \begin{bmatrix}
    \phi_{n1} \\
    \phi_{n2} \\
    \vdots \\
    \phi_{nn}
    \end{bmatrix}$ and $X_n = \begin{bmatrix}
    X_n \\
    X_{n-1} \\
    \vdots \\
    X_1
    \end{bmatrix}$\\

    Suppose $\{X_t\}$ follows causal AR$(p)$ process: $\phi(B) X_t = Z_t$ where $\{Z_t\} \sim WN(0, \sigma^2)$\\
    Predict $X_{t}$ based on $X_{t-1}, \ldots, X_{t-p}$ Then use $\phi_p = \begin{bmatrix}
    \phi_1 \\
    \phi_2 \\
    \vdots \\
    \phi_p
    \end{bmatrix}$  
\end{remark}
\begin{definition}[Partial Autocorrelation Function]
    LLet $\{X_t\}$ be a mean-zero stationary process. Its partial autocorrelation function (PACF) $\alpha(\cdot)$ is a function on positive integers defined as follows:
    \begin{itemize}
        \item $\alpha(1) = \rho(1)$
        \item $\alpha(k) = \text{Cor}[X_{k+1} - \mathcal{P}_{\overline{sp} \{X_k, \ldots, X_2\}} (X_{k+1}), X_1 - \mathcal{P}_{\overline{sp} \{X_2, \ldots, X_k\}} (X_1)]$ for $k \geq 2$
    \end{itemize}
\end{definition}
\begin{example}[3.4.1]
    AR$(1)$ process: $X_t = \phi X_{t-1} + Z_t$ 
    \begin{align*}
        \alpha(1) &= \rho(1) = \phi\\
        \alpha(2) &= \text{Cor}[X_3 - \mathcal{P}_{\spn\{X_2\}}(X_3), X_1 - \mathcal{P}_{\spn\{X_2\}}(X_1)]\\
        &= \text{Cor}[X_3 - \phi X_2, X_1 - \phi X_2]\\
        &= \text{Cor}[Z_3, X_1 - \phi X_2] = 0\\
        \alpha(k) &= 0 \text{ for } k > 1
    \end{align*}
\end{example}
\begin{example}[3.4.2]
    $MA(1)$ process: $X_t = Z_t + \theta Z_{t-1}$ where $\{Z_t\} \sim WN(0, \sigma^2)$
    \begin{align*}
        \alpha(1) &= \rho(1) = \frac{\theta}{1 + \theta^2}\\
        \alpha(2) &= \text{Cor}[X_3 - \mathcal{P}_{\spn\{X_2\}}(X_3), X_1 - \mathcal{P}_{\spn\{X_2\}}(X_1)]\\
        &= \text{Cor}[X_3 - \rho(1) X_2, X_1 - \rho(1) X_2]\\
        &= \frac{-\theta^2}{1 + \theta^2 + \theta^4}
    \end{align*}
\end{example}
\begin{definition}[Partial Least Squares]
Model 1: $y_i = \beta_0 + \sum_{j=1}^{p} \beta_j x_{ij} + \epsilon_i$ for $i = 1, \ldots, n$\\
Model 2: $y_i = \beta_0 + \sum_{j=1}^{k} \beta_j z_{ij} + \epsilon_i$ for $i = 1, \ldots, n$\\
PLS procedure:
Set LSE $\hat{\beta}_{(p-1)}$ based on Model 2
\begin{itemize}
    \item regress $\underline{x_p}$ on $\underline{x_1}, \ldots, \underline{x_{p-1}}$ set the LSE to $\hat{\gamma}$ ie $\underline{x_p} = \sum_{j=1}^{p-1} \gamma_j \underline{x_j} + \underline{z_p}$
    \item Regress $\underline{y}$ on $\underline{z_p}$, minimize $||\underline{y} - c \underline{z}_{p-1}||$ 
    $\hat{c} = \frac{\innerprod{y}{z_p}}{\innerprod{z_p}{z_p}}$
    \item get the LSE $\hat{\beta}_p$ BAsed on Model 1
    \item See that $\hat{c} = \hat{\beta}_p $
\end{itemize}
\end{definition}
\begin{remark}
    Prediction probkem 1:\\
    Predict $X_k$ using $X_2, \ldots, X_{k-1}$\\
    Prediction problem 2:\\
    Predict $X_k$ using $X_{k-1}, \ldots, X_2, X_1$\\
    Question how much does $X_1$ help in predicting $X_k$ given $X_2, \ldots, X_{k-1}$\\
    use $\frac{\innerprod{X_1 - \mathcal{P}_{\spn\{X_2, \ldots, X_{k-1}\}}(X_1)}{X_{k
    +1} - \mathcal{P}_{\spn\{X_2, \ldots, X_{k-1}\}}(X_{k+1})}}{|| X_1 - \mathcal{P}_{\spn\{X_2, \ldots, X_{k-1}\}}(X_1)||^2}$
    $w_1 = X_1 - \mathcal{P}_{\spn\{X_2, \ldots, X_{k-1}\}}(X_1)$\\
    $w_1 = X_1 - \phi_{k-1,1} X_2 - \ldots - \phi_{k-1,k-1} X_{k}$
    \begin{align*}
        \mathcal{P}_{\spn\{X_k \ldots X_1\}}(X_{k+1}) &= \mathcal{P}_{\spn\{X_k \ldots X_2\}}(X_{k}) + \mathcal{P}_{\spn\{w_1\}}(X_{k+1}) \text{ where } w_1 = X_1 - \mathcal{P}_{\spn\{X_2, \ldots, X_{k-1}\}}(X_1)\\
        &= \phi_{k-1,1} X_k + \ldots + \phi_{k-1,k-1} X_2 + \hat{c} \left(X_1 - \phi_{k-1,1} X_2 - \ldots - \phi_{k-1,k-1} X_k\right)\\
        &= \hat{c} X_1 + \ldots  
        &= \phi_{k,k} X_1 + \ldots 
    \end{align*}
    Thus $\alpha(k) = \phi_{kk}$
\end{remark}
\begin{example}
    AR$(p)$ process: $\phi(B) X_t = Z_t$ 
    \begin{align*}
        \alpha(1) &= \rho(1) = \phi_1\\
        \alpha(p) &= \phi_p\\
        \alpha(k) &= 0 \text{ for } k > p 
    \end{align*}
\end{example}


\begin{table}[ht]
\centering
\caption{Theoretical behaviour of the ACF and PACF for common linear time series models}
\label{tab:acf_pacf}
\begin{tabular}{p{2.5cm} p{5cm} p{5cm} p{3.5cm}}
\hline
Model & ACF behaviour & PACF behaviour & Identification rule (sample) \\
\hline
AR($p$) & Tails off (exponential or damped sinusoid depending on roots). & Cuts off after lag $p$ (i.e. $\alpha(k)\approx 0$ for $k>p$). & If sample ACF decays and sample PACF shows a clear cutoff at lag $p$, prefer AR($p$). \\
MA($q$) & Cuts off after lag $q$ (theoretical autocorrelations $\rho_k=0$ for $k>q$). & Tails off (no finite cutoff). & If sample ACF has significant spikes up to lag $q$ then $\approx 0$ afterwards, prefer MA($q$). \\
ARMA($p,q$) & Tails off (mixture-shaped decay from both AR and MA parts). & Tails off (mixture-shaped). & If both sample ACF and PACF tail off (no short cutoff), try ARMA($p,q$) and select $(p,q)$ by AIC/BIC. \\
\hline
\end{tabular}
\end{table}


\subsection{10/9/2025 Lecture 11}
\begin{remark}[Estimating The PACF]
    Assume $\{X_t\}$ is a mean-zero stationary process . \\
    Predict $X_{k+1}$ based on $X_k, \ldots, X_1$\\
    Then the coeeficient of $X_1$  is $\alpha(k)$\\
\textbf{Method 1} $\underline{\phi}_k = \Gamma_k^{-1} \underline{\gamma}_{k+1}$ where $\underline{\gamma}_{k} = [\gamma(1), \ldots, \gamma(k)]'$ and $\Gamma_k = [\gamma(i-j)]_{i,j=1}^{k}$\\
Plug in the sample autocovariance function $\hat{\gamma}(h)$ to get an estimated $\hat{phi_k}$ and take $\hat{\alpha}(k) = \hat{\phi}_{kk}$\\
\textbf{Method 2} $\min_{\phi_1, \ldots, \phi_k} \sum_{t= k+1}^{n} (X_t - \mathcal{P}_{\spn\{X_{k+1}, \ldots, X_1\}}(X_t))^2$\\
$ = \sum_{t=k+1}^{n} (X_t - \phi_{k1} X_{t-1} - \ldots - \phi_{kk} X_{t-k})^2$\\
Then take $\hat{\alpha}(k) = \hat{\phi}_{k}$
\end{remark}
\begin{remark}[ACF and PACF Estimation for ARMA$(p,q)$]
    Assume $\{X_t\}$ is a mean-zero stationary process and a causal and invertable ARMA$(p,q)$ process.\\
    \textit{How to calculate the ACF?}\\
    \begin{itemize}
        \item AR(1): $X_t = \phi X_{t-1} + Z_t$\\
        $\gamma(0) = \phi^2 \gamma(0) + \sigma^2 \implies \gamma(0) = \frac{\sigma^2}{1 - \phi^2}$ note that it needs to be causal so $|\phi| < 1$.\\
        $\gamma(h) = \phi \gamma(h-1) $ for $h \geq 1$\\
        $\implies \gamma(h) = \phi^{|h|} \gamma(0)$\\
        \item AR(2): $X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + Z_t$\\
        covariance with $X_{t-1}$ is $\gamma(1) = \phi_1 \gamma(0) + \phi_2 \gamma(1) \implies \rho(1) = \phi_1 + \phi_2 \rho(1) \implies \rho(1) = \frac{\phi_1}{1 - \phi_2}$\\
        $\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$ for $h \geq 2 \implies \rho(h) = \phi_1 \rho(h-1) + \phi_2 \rho(h-2)$ for $h \geq 2$
        \item General ARMA$(p,q)$: \\
        Method 1: Write $X_t = \frac{\theta(B) Z_t}{\phi(B)} = \psi(B) Z_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j}$\\
        $\gamma(0) = \psi_0^2 \sigma^2 + \psi_1^2 \sigma^2 + \ldots = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$\\
        $\gamma(h) = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+|h|}$ for $h \geq 1$\\
    \end{itemize}
    $\psi(z) = \frac{\theta(z)}{\phi(z)} \implies \phi(z) \psi(z) = \theta(z)$\\
    $(1- \phi_1 z - \ldots - \phi_p z^p)(\psi_0 + \psi_1 z + \psi_2 z^2 + \ldots) = 1 + \theta_1 z + \ldots + \theta_q z^q$\\
    Match the coefficients of $z^j$ for $j = 0, 1, 2, \ldots$ to get $\psi_0, \psi_1, \ldots$\\
    \begin{itemize}
        \item $\psi_0 = 1$
        \item $\psi_1 - \phi_1 \psi_0 = \theta_1 \implies \psi_1 = \phi_1 + \theta_1$
        \item $\psi_2 - \phi_1 \psi_1 - \phi_2 \psi_0 = \theta_2 \implies \psi_2 = \phi_1 \psi_1 + \phi_2 + \theta_2$
    \end{itemize}
    Method 2: Example ARMA(2,2) $X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + Z_t + \theta_1 Z_{t-1} + \theta_2 Z_{t-2}$\\
    Covariance with $X_{t}$: $\gamma(0) = \phi_1 \gamma(1) + \phi_2 \gamma(2) + \sigma^2 (1 + \theta_1\psi_1 + \theta_2\psi_2)$\\
    Covariance with $X_{t-1}$: $\gamma(1) = \phi_1 \gamma(0) + \phi_2 \gamma(1) + \sigma^2 (\theta_1 + \theta_2 \psi_1)$\\
    Covariance with $X_{t-2}$: $\gamma(2) = \phi_1 \gamma(1) + \phi_2 \gamma(0) + \sigma^2 \theta_2$\\
    Now we can solve for $\gamma(0), \gamma(1), \gamma(2)$\\
    Covariance with $X_{t-h}$ for $h \geq 3$: $\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$\\
    Method 2.1: Solve the Difference Equation\\
    Method 2.2: Get $\gamma(h)$ recursively \\
    In general ARMA$(p,q)$: make a system of $p+1$ equations. 
\end{remark}
\begin{remark}[Statistical infrence]
    Ask the questions: How do I know the order of the model? How do I estimate the parameters? How do I check if the model is good?\\
\end{remark}
\begin{remark}[Spectral Representaions of Staitionary Processes]
    New Chapter
\end{remark}
\begin{definition}[Complex Random Variable]
    $X = Re(X) + i Im(X)$ where $Re(X), Im(X)$ are real random variables.\\
    or $X = X_1 + iX_2$ where $X_1, X_2$ are real random variables on the same probability space.\\
    \textbf{Properties}\\
    $E[X] = E[X_1] + i E[X_2]$\\
    Suppose $Y = Y_1 + i Y_2$ is another complex random variable.\\
    $\cov(X, Y) = E[(X - E[X]) \overline{(Y - E[Y])}]$ \\
    $\cov(X, Y) = \overline{\cov(Y, X)}$\\
    Assume $E[X] = 0$ then $\var(X) = E[(X+ iX_2)(X - iX_2)] = E[X_1^2 + X_2^2] \geq 0$\\
    To get Properties of $X_1, X_2$ from $X$ use Variance and Second moment\\
    $L^2_c(\Omega, \mathcal{F}, P) = \{X: X = X_1 + iX_2, X_1, X_2 \in L^2(\Omega, \mathcal{F}, P)\}$
\end{definition}
\begin{definition}[Stationary Process for Complex Random Variables]
    $\{X_t\}$ is a complex-valued process.\\
    It is a complex valed staition ary process if $E[|X_t|^2] < \infty$ and $E[X_t]$ does depend on $t$ and $E[X_{t+h} \overline{X_t}]$ does not depend on $t$.\\
    Write $X_t = X_{t1} + i X_{t2}$ where $X_{t1}, X_{t2}$ are real-valued processes.\\
    \begin{align*}
        E[X_{t+h} \overline{X_t}] &= E[(X_{t+h,1} + i X_{t+h,2})(X_{t1} - i X_{t2})] \\
        &= E[X_{t+h,1} X_{t1}] + E[X_{t+h,2} X_{t2}] + i (E[X_{t+h,2} X_{t1}] - E[X_{t+h,1} X_{t2}])
    \end{align*}
    This means the sum of the auto and cross covariance functions do not depend on $t$.\\
    \fbox{If a process is complex-valued stationary process does it imply that the real and imaginary parts are stationary? NO}
\end{definition}
\subsection{10/14/2025 Lecture 12}
\begin{remark}
    Review of last class:
    \begin{itemize}
        \item $X = X_1 + i X_2$ where $X_1, X_2$ are real random variables
        \item $E[X] = E[X_1] + i E[X_2]$
        \item $\innerprod{X}{Y} = E[X \overline{Y}]$
        \item $\cov(X, Y) = E[(X - E[X]) \overline{(Y - E[Y])}] = \innerprod{X}{Y} - \innerprod{E[X]}{E[Y]}$
        \item Complex $L^2$ space
        \item Complex-valued stationary process:
        \begin{itemize}
            \item $E[|X_t|^2] < \infty$
            \item $E[X_t]$ does not depend on $t$
            \item $E[X_{t+h} \overline{X_t}]$ does not depend on $t$
        \end{itemize}
        \item the autocovariance of a complex-valued stationary process is $\gamma(h) = E[X_{t+h} \overline{X_t}] = \innerprod{X_{t+h}}{X_t} - \innerprod{E[X_{t+h}]}{E[X_t]}$. \item $\gamma(h) = \overline{\gamma(-h)}$ it is hermitian
    \end{itemize}
\end{remark}
\begin{theorem}[4.1.1]
    A function $k(\cdot)$ defined on the integers is an autocovariance function of a complex valued stationary process if and only if it is non-negative definite.\\
    IE $\sum_{j,k =1}^{n} a_j k(j-k) \overline{a_k} \geq 0$ for all $n \geq 1$ and $a_1, \ldots, a_n \in \mathbb{C}$\\
    \textit{This also implies that the acf is also hermitian}
    \begin{proof}
        $\implies$: \fbox{Left As excerise}\\
        $\impliedby$: NND when $n=1$ $a_1=1$ implies $k(0) \geq 0$\\
        NND when $n=2$ take $a_1 = 1$, $k(0) + k(0)\cdot|a_2|^2 + k(-1) \overline{a_2} + k(1) a_2 \geq 0$\\
        it is always real $\implies k(1) = -{k(-1)}$\\
        for an arbitrary $n$ take $a_j = 0$ for $j \neq 1, n$\\
        $a_1 = 1$, $k(1-n) = -k(n-1)$\\ 
        Suppose $k(\cdot)$ is the acf of $X_t = Y_t + i Z_t$ where $Y_t, Z_t$ are real-valued stationary processes with mean 0\\
        Let $\underbar{X} = (X_1, \ldots, X_n)'$ and $\underbar{X} = \underbar{Y} + i \underbar{Z}$\\
        $\cov(\underbar{X}) = E[\underbar{X} \underbar{X}^*] = E[(\underbar{Y} + i \underbar{Z})(\underbar{Y}' - i \underbar{Z}')] $\\
        $ = \Sigma_{yy} + \Sigma_{zz} + i (\Sigma_{zy} - \Sigma_{yz})$ \\
        $\cov(\underbar{X}) = \begin{bmatrix}
            k(0) & k(-1) & k(-2) & \ldots & k(1-n) \\
            k(1) & k(0) & k(-1) & \ldots & k(2-n) \\
            k(2) & k(1) & k(0) & \ldots & k(3-n) \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            k(n-1) & k(n-2) & k(n-3) & \ldots & k(0)
        \end{bmatrix} = K_1 + i K_2$ where $K_1$ and $K_2$ are real  matrices.\\
        write $\underbar{a} = (a_1, \ldots, a_n)' = \underbar{b} + i \underbar{c}$\\
        \begin{align*}
            \sum_{j,k=1}^{n} a_j k(j-k) \overline{a_k} &= \underbar{a}' \underbar{K} \overline{\underbar{a}}\\
            &= (\underbar{b} + i \underbar{c})' (K_1 + i K_2) (\underbar{b} - i \underbar{c})\\
            &= \underbar{b}' K_1 \underbar{b} + \underbar{c}' K_1 \underbar{c} + b' K_2 c - c' K_2 b \\
            \text{same as} &= (b_1' - c') \begin{bmatrix}
                k_1 &k_2'\\
                k_2 & k_1
            \end{bmatrix} \begin{bmatrix}
                b \\
                c
            \end{bmatrix} \geq 0
        \end{align*}
        Take $\Sigma_{yy} = \Sigma_{zz} = \frac{1}{2} K_1$ and $\Sigma_{zy} = \frac{1}{2} K_2$, $\Sigma_{yz} = \frac{1}{2} K_2' = -\frac{1}{2} K_2$\\
        $K_1' - iK_2' = K_1 + i K_2$ thus $K_2' = -K_2$\\
        Construct $\begin{bmatrix}
            \underbar{Y}\\
            \underbar{Z}
        \end{bmatrix} \sim N(0, \frac{1}{2} \begin{bmatrix}
            K_1 & K_2' \\
            K_2 & K_1
        \end{bmatrix})$
    \end{proof}
\end{theorem}
\begin{example}
    $X_t = \sum_{j=1}^n A_j e^{i t \lambda_j}$ 
    \begin{itemize}
        \item $-\pi < \lambda_1 < \lambda_2 < \ldots < \lambda_n = \pi$
        \item $A_j$ are complex-valued random variables
        \item $E[A_j] = 0$, $E[A_j \overline{A_k}] = 0$ for $j \neq k, E[|A_j|^2] = \sigma_j^2 < \infty$
    \end{itemize}
    Write $A_j = C_j + i D_j$ then
    \begin{align*}
        (C_j + iD_j)e^{i t \lambda_j} &= (C_j + i D_j)(\cos(t \lambda_j) + i \sin(t \lambda_j))\\
        &= (C_j \cos(t \lambda_j) - D_j \sin(t \lambda_j)) + i (C_j \sin(t \lambda_j) + D_j \cos(t \lambda_j))
    \end{align*}
    We can see that this is a complex-valued stationary process, but if I want the $X$ to be real then we require the conition that $C_j \sin(t \lambda_j) + D_j \cos(t \lambda_j) = 0$\\
    Want $X_t$ to be real valued $\begin{cases}
        \lambda_j = -\lambda_{n-j} \text{ for } j = 1, \ldots, n-1\\
        A(\lambda_j) = \overline{A(\lambda_{n-j})} \text{ for } j = 1, \ldots, n-1\\
        A_n \text{ is real}
    \end{cases}$\\
    This 
\end{example}
\subsection{10/16/2025 Lecture 13}
\begin{remark}
    Review of what we are looking at \\
    $$ X_t= \sum_{j=1}^n A_j e^{i t \lambda_j} $$
    where $\begin{cases}
        -\pi < \lambda_1 < \lambda_2 < \ldots < \lambda_n = \pi\\
        A_j \text{ are uncorrelated complex-valued random variables}\\
        E[A_j] = 0, E[A_j \overline{A_k}] = 0 \text{ for } j \neq k, E[|A_j|^2] = \sigma_j^2 < \infty
    \end{cases}$\\
    When is $X_t$ real-valued?
    $\begin{cases}
        \lambda_j = -\lambda_{n-j} \quad 1 \leq j \leq n-1\\
        A_j = \overline{A_{n-j}} \quad 1 \leq j \leq n-1\\
        A_n \text{ is real}
    \end{cases}$
    Is $X_t$ stationary?
    \begin{align*}
        E(X_{t+h} \overline{X_t}) &= E\left[
            \left(\sum_{j=1}^n A_j e^{i (t+h) \lambda_j}\right) \left(\sum_{k=1}^n \overline{A_k} e^{-i t \lambda_k}\right)
            \right]
        &= E\left[
            A_1 \overline{A_1} e^{i h \lambda_1} + A_2 \overline{A_2} e^{i h \lambda_2} + \ldots + A_n \overline{A_n} e^{i h \lambda_n}
            \right]\\
        &= \sum_{j=1}^n \sigma_j^2 e^{i h \lambda_j}
    \end{align*}
    This does not depend on $t$ so it is stationary.\\
    $\gamma(h) = \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu)$ where $F(\nu) = \sum_{j: \lambda_j \leq \nu} \sigma_j^2$ 
    This is the Reimann-Stieltjes integral
    This is a step function with jumps of size $\sigma_j^2$ at $\lambda_j$ for $j = 1, \ldots, n$ \\
    View $F(\nu)$ as a measure on $(-\pi, \pi]$ which assigns point measures $m(\nu) = \sigma_j^2$ \\
    The function $e^{i h \nu}$ takes on values $e^{i h \lambda_j}$ at the points $\lambda_j$ with value $\sigma_j^2$\\
    Every mean-zero stationary process $\{X_t\}$ has a representation
    $$ X_t = \int_{-\pi}^{\pi} e^{i t \nu} dZ(\nu) $$
    \fbox{If we have a continous path, everywhere differentialble, how is the different from the Riemann-Stieltjes}
    where $Z(\nu)$ is a complex-valued process with the following properties:
    \begin{itemize}
        \item $E[dZ(\nu)] = 0$
        \item $E[|dZ(\nu)|^2] = dF(\nu)$ where $F(\nu)$ is a non-decreasing function on $(-\pi, \pi]$ with $F(-\pi) = 0$ and $F(\pi) = \gamma(0)$
        \item $E[dZ(\nu) \overline{dZ(\lambda)}] = 0$ for $\nu \neq \lambda$
    \end{itemize}
    Correspondingly $\gamma_X(h) = \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu)$
    \textbf{Riemann integral}: $\int_{a}^{b} g(x) dx$ where $g(x)$ is a function on $[a,b]$\\
    $\lim_{\max |a_j - a_{j-1}| \to 0} \sum_{j=1}^{n} g(a_j) (a_j - a_{j-1})$\\
    $\lim_{\max |\lambda_j - \lambda_{j-1}| \to 0} \sum_{j=1}^{n} e^{i h \lambda_j} Z(\lambda_j) - Z(\lambda_{j-1})$\\
    
    $ F(\cdot)$ is called the spectral distribution function of $\{X_t\}$\\
    $F $ is increasing and caddlag, $F(-\pi) = 0$, $F(\pi) = \gamma(0)$\\
\end{remark}
\begin{theorem}[4.3.1 Herglotz Theorem]
    A complex Values fn $\gamma(\cdot)$ defined on the integers is NND if and only if
    \begin{align*}
        \gamma(h) &= \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu) \quad h \in \mathbb{Z}\\
    \end{align*}
    Where $F(\cdot)$ is a distribution function supported on $(-\pi, \pi]$ with $F(-\pi) = 0$ and $F(\pi) \leq \infty$\\
    \textit{Note}: Corresponding Bocher's for the characteristic function of a random variable\\
    If $F(\cdot)$ is absolutely Continuous w.r.t the Lebesgue measure, (ie they are related but not the same) ie $F(\lambda) = \int_{-\pi}^{\lambda} f(\nu) d\nu$ then $f(\lambda)$ is called the spectral density function of $\{X_t\}$\\
    \begin{proof}
        $\impliedby$ 
        \begin{align*}
            \sum_{j,k=1}^{n} a_j \gamma(j-k) \overline{a_k} &= \sum_{j,k=1}^{n} a_j \int_{-\pi}^{\pi} e^{i (j-k) \nu} dF(\nu) \overline{a_k}\\
            &= \int_{-\pi}^{\pi} \sum_{j,k=1}^{n} a_j e^{i (j) \nu} e^{-i k \nu} \overline{a_k} dF(\nu)\\
            &= \int_{-\pi}^{\pi} \left( \sum_{j=1}^{n} a_j e^{i j \nu} \right) \left( \sum_{k=1}^{n} \overline{a_k e^{i k \nu}} \right) dF(\nu)\\
            &= \int_{-\pi}^{\pi} \left| \sum_{j=1}^{n} a_j e^{i j \nu} \right|^2 dF(\nu) \geq 0
        \end{align*}
    \end{proof}
\end{theorem}
\subsection{10/21/2025 Lecture 14}
\begin{remark}[Spectral Representation of Complex Stationary Process]
    Every zero-mean stationary process has a representation:
    $$ X_t = \int_{-\pi}^{\pi} e^{i t \nu} dZ(\nu) $$
    Correspondingly $\gamma_X(h) = \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu)$
    F is cadlag, increasing, $F(-\pi) = 0$, $F(\pi) = \gamma(0)$\\
    where $Z(\nu)$ is a complex-valued process with the following properties:\\
    If $F(\cdot)$ is absolutely continuous w.r.t the Lebesgue measure, ie $F(\lambda) = \int_{-\pi}^{\lambda} f(\nu) d\nu$ then $\gamma_X(h) = \int_{-\pi}^{\pi} e^{i h \nu} f(\nu) d\nu$ where $f(\nu)$ is called the spectral density function of $\{X_t\}$\\
    F is the spectral distribution function of $\{X_t\}$
    f is the spectral density function of $\{X_t\}$
\end{remark}
\begin{theorem}[4.3.1 Herglotz]
    A complex valued fn $\gamma(\cdot)$ defined on $\mathbb{Z}$ is NND if and only if
    \begin{align*}
        \gamma(h) &= \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu) \quad h \in \mathbb{Z}
    \end{align*}
    Where $F(\cdot)$ is a distribution function supported on $(-\pi, \pi]$ with $F(-\pi) = 0$ and $F(\pi) \leq \infty$\\
    \begin{proof}
        $\implies:$ Define $f_n(\nu) = \frac{1}{2\pi N} \sum_{j,k=1}^{N} \gamma(j-k) e^{-ij \nu} e^{i k \nu}$
        \begin{align*}
            f_n(\nu) &= \frac{1}{2\pi N} \sum_{j,k=1}^{N} \gamma(j-k) e^{-ij \nu} e^{i k \nu}\\
            &= \frac{1}{2\pi N} \sum_{m=1-N}^{N-1} (1-\frac{|m|}{N})\gamma(m) e^{i m \nu} 
        \end{align*}
        \fbox{Fejer Kernel}
        Then $f_n(\nu) \geq 0$ and $\int_{-\pi}^{\pi} f_n(\nu) d\nu = \gamma(0)$\\
        Next Define $F_N(\lambda) =  \int_{-\pi}^{\lambda} f_n(\nu) d\nu$ Then $F_N(\lambda)$ is a distribution function on $(-\pi, \pi]$ with $F_N(-\pi) = 0$ and $F_N(\pi) = \gamma(0)$\\
        And
        \begin{align*}
            \int_{-\pi}^{\pi} e^{i h \nu} dF_N(\nu) &= \int_{-\pi}^{\pi} e^{i h \nu} f_n(\nu) d\nu\\
            &= \begin{cases}
                \gamma(h) (1 - \frac{|h|}{N}) & |h| < N\\
                0 & |h| \geq N
            \end{cases}
        \end{align*}
        By Helly's Theorem: there exists a subsequence $\{N_k\}$ s.t. $F_{N_k}(\cdot) \to F(\cdot)$ and $ \lim_{k \to \infty} \int_{-\pi}^{\pi} e^{i h \nu} dF_{N_k}(\nu) = \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu)$\\
        \begin{align*}
            \lim_{k \to \infty} \int_{-\pi}^{\pi} e^{i h \nu} dF_{N_k}(\nu) &= \lim_{k \to \infty} \gamma(h) (1 - \frac{|h|}{N_k}) = \gamma(h)\\
        \end{align*}
    \end{proof}
\end{theorem}
\begin{remark}
    F as the limit might have a point mass at $-\pi$ so an additial step transfer it to $\pi$\\
    It can be proven that $F_N(\cdot) \to F(\dot)$ \\
    This also ensure the spectral distribution function is unique ie if $\exists$ another distribution function $G(\cdot)$ s.t $\gamma(h) = \int_{-\pi}^{\pi} e^{i h \nu} dG(\nu)$ then $F(\cdot) = G(\cdot)$ at all continuity points of both $F$ and $G$\\
\end{remark}
\begin{theorem}[4.3.2]
    Suppose $K(\cdot)$ is a function defined on $\mathbb{Z}$ s.t. $\sum_{h=-\infty}^{\infty} |K(h)| < \infty$ then \\
    $$ f(\lambda) := \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} K(h) e^{-i h \lambda} $$
    is well -defined and $K(n) = \int_{-\pi}^{\pi} e^{i n \lambda} f(\lambda) d\lambda$ for all $n \in \mathbb{Z}$\\
\end{theorem}
\begin{corollary}
    Suppose $\gamma(\cdot)$ is a function defined on $\mathbb{Z}$ and it $ \sum_{h=-\infty}^{\infty} |\gamma(h)| < \infty $ then $\gamma(\cdot)$ is the acf of a stationary process iff $f(\lambda) := \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} \gamma(h) e^{-i h \lambda} \geq 0$ for all $\lambda \in [-\pi, \pi]$\\
    \begin{proof}
        $\impliedby$: f is a density, can define $F(\lambda) = \int_{-\pi}^{\lambda} f(\nu) d\nu$ which is a distribution function on $(-\pi, \pi]$\\
        $\implies$ $f_N(\lambda) = \frac{1}{2\pi} \sum_{m=-1-N}^{N-1} (1 - \frac{|m|}{N}) \gamma(m) e^{-i m \lambda} \geq 0$ for all $\lambda \in [-\pi, \pi]$\\
    \end{proof}
\end{corollary}
\begin{example}[4.3.1]
    $K(h) = \begin{cases}
        1 & h = 0\\
        \rho & h = \pm 1\\
        0 & \text{otherwise}
    \end{cases}$ where $|\rho| < \frac{1}{2}$\\
    We can look at the spectral density function:
    \begin{align*}
        f(\lambda) &= \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} K(h) e^{-i h \lambda} \\
        &= \frac{1}{2\pi} (1 + \rho e^{-i \lambda} + \rho e^{i \lambda})\\
        &= \frac{1}{2\pi} (1 + 2 \rho \cos(\lambda))
    \end{align*}
\end{example}
\begin{remark}
    If the acf is real, then the spectral density function is also real-valued.\\
    $f(\lambda) = \frac{1}{2\pi} \left[ 1 + \sum_{n=1}^{\infty} 2 \gamma(n) \cos(n \lambda) \right]$\\
    In general the spectral distribution function is stne=netruc in the sense $F_X(\lambda) = F_X(\pi^-) - F_X(-\lambda^-)$\\
    \fbox{HW}
\end{remark}
\subsection{10/23/2025 Lecture 15}
\begin{remark}
    If $\gamma(\cdot)$ is a complex balued acg then there exists a unique spectral distribution $F$ s.t. $\gamma(h) = \int_{-\pi}^{\pi} e^{i h \nu} dF(\nu)$\\
    If $F(\cdot)$ is absolutely continuous w.r.t the Lebesgue measure ie $F(\lambda) = \int_{-\pi}^{\lambda} f(\nu) d\nu$ then $\gamma(h) = \int_{-\pi}^{\pi} e^{i h \nu} f(\nu) d\nu$ where $f(\nu)$ is called the spectral density function of $\{X_t\}$\\
    If $\sum_{h=-\infty}^{\infty} |\gamma(h)| < \infty$ then he spectral density eists and is given by $f(\lambda) = \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} \gamma(h) e^{-i h \lambda}$\\
    Consider a $WN(0, \sigma^2)$, the acf is $\gamma(h) = \begin{cases}
        \sigma^2 & h = 0\\
        0 & h \neq 0
    \end{cases}$\\
    The spectral density function is $f(\lambda) = \frac{1}{2\pi} \sigma^2$ for $\lambda \in [-\pi, \pi]$\\
\end{remark}
\begin{theorem}[4.4.1]
    If $\{Y_t\}$ is zero-mean complex valued stationary process with the spectral denisty $F_Y(\cdot)$ and if $\{X_t\}$ is defined by $X_t = \sum_{j=-\infty}^{\infty} \psi_j Y_{t-j}$ where $\sum_{j=-\infty}^{\infty} |\psi_j| < \infty$ then $\{X_t\}$ is staitionary with acf $\gamma_X(h) = \sum_{j,k = -\infty}^{\infty} \psi_j \overline{\psi_k} \gamma_Y(h - j + k)$ 
    AAnd the spectral distribution: $F_X(\lambda) = \int_{-\pi}^{\lambda} |\sum_{j=-\infty}^{\infty} \psi_j e^{-i j \nu}|^2 dF_Y(\nu)$\\
    This a Radon-Nikodym derivative\\
    \begin{proof}
        \begin{align*}
            \gamma_X(h) &= \sum_{j,k = -\infty}^{\infty} \psi_j \overline{\psi_k} \int_{-\pi}^{\pi} e^{i (h - j + k) \nu} dF_Y(\nu)\\
            &= \int_{-\pi}^{\pi} e^{i h \nu} \left[ \sum_{j,k = -\infty}^{\infty} \psi_j e^{-i j \nu} \overline{\psi_k e^{-i k \nu}} \right] dF_Y(\nu)\\
            &= \int_{-\pi}^{\pi} e^{i h \nu} |\sum_{j=-\infty}^{\infty} \psi_j e^{-i j \nu}|^2 dF_Y(\nu)\\
            &= \int_{-\pi}^{\lambda} e^{i h \nu} dF_X(\nu) \quad \text{ where } F_X(\lambda) = \int_{-\pi}^{\lambda} |\sum_{j=-\infty}^{\infty} \psi_j e^{-i j \nu}|^2 dF_Y(\nu)
        \end{align*}
    \end{proof}
\end{theorem}
\begin{theorem}[~3.1.3]
    Consider $ARMA(p,q)$ $\phi(B) X_t = \theta(B) Z_t$ If $\phi(z)\neq 0$ for $|z| = 1$ then there is a unique stationary solution given by $X_t = \sum_{j=\infty}^{\infty} \psi_j Z_{t-j}$ where $\psi(z) = \frac{\theta(z)}{\phi(z)} = \sum_{j=-\infty}^{\infty} \psi_j z^j$ for $|z| \leq 1$\\
    note:\\
    $\phi(z) = (1-.5 z)(1 - 2 z)$\\
    $1/ \phi(z) = 1/(1 - .5 z) \cdot 1/(1 - 2 z)$\\
    $\frac{1}{1-2z} = \frac{1}{z} \cdot \frac{1}{\frac{1}{z}-2}$
\end{theorem}
\begin{theorem}[4.4.2]
    Consider the $ARMA(p,q)$ $\phi(B) X_t = \theta(B) Z_t$ Assume that $\phi, \theta$ have no common zeros and that $\phi(z) \neq 0$ for $|z| = 1$ then $\{X_t\}$ has a spectral density function given by
    \begin{align*}
        f_X(\lambda) &= \frac{\sigma^2}{2\pi} \cdot \frac{|\theta(e^{-i \lambda})|^2}{|\phi(e^{-i \lambda})|^2} \quad \lambda \in [-\pi, \pi]
    \end{align*}
    since $dF_Z(\lambda) = \frac{\sigma^2}{2\pi} d\nu$ 
    \begin{proof}
        Define $U_t = \phi(B) X_t = \theta(B) Z_t$ \\
        $f_U(\lambda) = |\theta(e^{-i \lambda})|^2 f_Z(\lambda) = \frac{\sigma^2}{2\pi} |\theta(e^{-i \lambda})|^2$\\
        And thus $\implies: f_X(\lambda) = \frac{\sigma^2}{2\pi} \cdot \frac{|\theta(e^{-i \lambda})|^2}{|\phi(e^{-i \lambda})|^2}$\\
    \end{proof}
\end{theorem}
\begin{example}
    $AR(1): X_t = \phi X_{t-1} + Z_t$ where $|\phi| \neq 1$\\
    $f_X(\lambda) = \frac{\sigma^2}{2\pi} \cdot \frac{1}{|1 - \phi e^{-i \lambda}|^2} = \frac{\sigma^2}{2\pi} \cdot \frac{1}{1 - 2 \phi \cos(\lambda) + \phi^2}$\\
    Suppose $|\phi| > 1$ We want to have $(1-\frac{1}{\phi} B)X_t = Z_t^*$ ?. \\
    \begin{align*}
        (1-\frac{1}{\phi}B)(1-\phi B) X_t &= (1 - \frac{1}{\phi} B) Z_t\\
        (1 - \frac{1}{\phi}B) X_t = \frac{(1 - \frac{1}{\phi}B)}{(1-\phi B)} Z_t &= Z_t^*\\
    \end{align*}
    Thus by Theorem $4.4.2$ the spectral density function is $f_{Z^*}(\lambda) = \left|\frac{1-\frac{1}{\phi} e^{-i \lambda}}{1-\phi e^{-i \lambda}} \right| \frac{\sigma^2}{2\pi}$
    We can verify that this is a constant by 
    \begin{align*}
        |e^{i \lambda}e^{-i \lambda} - \frac{1}{\phi} e^{i \lambda}|^2 &= |e^{i \lambda} - \frac{1}{\phi}|^2\\
        &= \frac{1}{\phi^2} |-\phi e^{-i \lambda} + 1|^2 \quad \text{Which cancels with the denominator}
    \end{align*}
    Thus $f_{Z^*}(\lambda) = \frac{\sigma^2}{2\pi \phi^2}$ for $\lambda \in [-\pi, \pi]$\\
    IE $Z_t^* \sim WN(0, \frac{\sigma^2}{\phi^2})$
\end{example}
\begin{example}
    $MA(1): X_t = Z_t + \theta Z_{t-1}$ where $Z_t \sim WN(0, \sigma^2)$, $|\theta| > 1$\\
    $X_t = (1 + \theta B) Z_t = (1+\frac{1}{\theta} B) Z_t^*$\\
    $\implies Z_t^* = \frac{(1+\theta B)}{(1+\frac{1}{\theta} B)} Z_t$\\
    Thus $Z_t^* \sim WN(0, \sigma^2 \theta^2)$ 
\end{example}
\begin{remark}
    Suppose $\{X_t\}$ iscausal and invertable. LEt $\mathcal{H}_t = \spn\{X_t, X_{t-1}, \ldots\}$. \\
    \begin{align*}
        X_t - \mathcal{P}_{\mathcal{H}_{t-1}} X_t = Z_t
    \end{align*}
    Since\\
    Causality implies that $\mathcal{H}_{t-1} \subset \spn\{Z_{t-1}, Z_{t-2}, \ldots\} \implies Z_t \perp \mathcal{H}_{t-1}$\\
    Invertability implies that $Z_{t-1}, Z_{t-2}, \ldots \in \mathcal{H}_{t-1}$\\
\end{remark}
\subsection{10/28/2025 Lecture 16}
\begin{remark}
    Suppose $\phi(B)X_t = \theta(B)Z_t$ $\phi, \theta$ have no common zeros and $\phi(z) \neq 0$ for $|z| = 1$\\
    The spectral of $X_t$ is given by $f_X(\lambda) = \frac{\sigma^2}{2\pi} \cdot \frac{|\theta(e^{-i \lambda})|^2}{|\phi(e^{-i \lambda})|^2}$\\
\end{remark}
\begin{example}[AR(1) Spectral density]
    $X_t = \phi X_{t-1} + Z_t$ where $|\phi| > 1$ then $X_t = \frac{1}{\phi} X_{t-1} + Z_t^*$ where $Z_t^* \sim WN(0, \frac{\sigma^2}{\phi^2})$\\ 
    And $Z_t^* = \frac{(1 - \frac{1}{\phi} B)}{(1 - \phi B)} Z_t$\\
\end{example}
\begin{example}[MA(1) Spectral density]
    $X_t = Z_t + \theta Z_{t-1}$ where $|\theta| > 1$ then $X_t = (1 + \frac{1}{\theta} B) Z_t^*$ where $Z_t^* \sim WN(0, \sigma^2 \theta^2)$\\
    And $Z_t^* = \frac{(1 + \theta B)}{(1 + \frac{1}{\theta} B)} Z_t$\\
\end{example}
\begin{definition}
    Suppose $\phi(B)X_t = \theta(B) Z_t$ is causal and invertable $\mathcal{M}_t = \spn\{X_t, X_{t-1}, \ldots\}$\\
    The BLP $\mathcal{P}_{\mathcal{M}_{t-1}} X_t$ what is $X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t$?\\
    Causal - $Z_t \perp M_{t-1}$\\
    Invertable - $Z_{t-1}, Z_{t-2}, \ldots \in \mathcal{M}_{t-1}$\\
    Thus $X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t = Z_t$
    Consequently $||X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t||^2 = E[|Z_t|^2] = \sigma^2$\\
    Thus for AR(1) with $|\phi| > 1$, the BLP error variance is $\sigma^2/\phi^2$\\
    For MA(1) with $|\theta| > 1$, the BLP error variance is $\sigma^2 \theta^2$\\
\end{definition}
\begin{remark}
    How do you make an ARMA that is not causal or invertable into one that is?\\
    More generally write $\phi(z) = \prod_{j=1}^{m} (1 - a_j^{-1}z) $ where $|a_j| >1$ for $1 <j < r$ and $|a_j| < 1$ for $r+1 < j < m$\\
    \begin{align*}
        (1-a_1^{-1} z)(1- a_2^{-1} z) \ldots (1 - a_r^{-1} z) (1 - a_{r+1}^{-1} z) \ldots (1 - a_m^{-1} z)
    \end{align*}
    The left part is the $(1-a_{r+1}z) / (1 - a_{r+1}^{-1})z$\\
    \begin{align*}
        \frac{1-\overline{a_{r+1}}B}{1+ a_{r+1}^{-1} B} \phi(B) X_t = \frac{1 - \overline{a_{r+1}} B}{1 - a_{r+1}^{-1} B} \theta(B) Z_t
    \end{align*}
    $a_{r+1} = ce^{i \theta}$ for $c < 1$ \\
    Can verify that $\frac{1-a_{r+1}B}{1-a_{r+1}^{-1} B} Z_t \sim WN(0, |a_{r+1}|^2 \sigma^2)$
    Then do the same thing for every $r+1 \leq j \leq m$ ie replace $a_{r+1}$ with somehting that makes it causal and invertable\\
    Thus 
    \begin{align*}
        \hat \phi(z) = \frac{j = r+1 }{p} \frac{1-\overline{a_j}B}{1 - a_j^{-1} B} \phi(z)\\
    \end{align*}
    Thus 
    \begin{align*}
        Z_t^* = \prod_{j=r+1}^{p} \frac{1 - \overline{a_j} B}{1 - a_j^{-1} B} Z_t \sim WN(0, \sigma^2 \prod_{j=r+1}^{p} |a_j|^2)\\
    \end{align*}
    Also 
    \begin{align*}
        \theta(z) = \prod_{j=1}^{q} (1 - b_j^{-1} z) \text{ where } |b_j| > 1 \text{ for } 1 \leq j \leq s \text{ and } |b_j| < 1 \text{ for } s+1 \leq j \leq q
    \end{align*}
    Then 
    \begin{align*}
        \hat \theta(z) = \prod_{j=s+1}^{q} \frac{1 - \overline{b_j} B}{1 - b_j^{-1} B} \theta(z)\\
    \end{align*}
    More Generally $\phi(B)X_t = \theta(B) Z_t$ When conveted 
    \begin{align*}
        \var (Z_t^*) = \sigma^2 \frac{\prod_{j=r+1}^{p} |a_j|^2}{\prod_{j=s+1}^{q} |b_j|^2}
    \end{align*}
    This is a theorem in the Book. \\
    Whenever there is a root that is bad, we can flip it by takeing the conjugate reciprocal and adjusting the variance accordingly.\\
\end{remark}
\begin{proposition}[4.4.1]
    Assume ARMA as usual. and $\theta(z) \neq 0$ for $|z| <1 $ then $Z_t \in \spn\{X_t, X_{t-1}, \ldots\}$\\
    \begin{proof}
        First: Factorize $\theta(z) = \theta^+(z) \theta^*(z)$ where $\theta^+(z) = \prod_{j=1}^{s} (1 - b_j^{-1} z)$ with $|b_j| > 1$ and $\theta^*(z) = \prod_{j=s+1}^{q} (1 - b_j^{-1} z)$ with $|b_j| = 1$\\
        Define $Y_t = \theta^*(B) Z_t$ then $\phi(B) X_t= \theta^+(B) Y_t$\\
        Using the earlier proof on invertability, $\implies Y_t \in \spn\{X_t, X_{t-1}, \ldots\}$\\
        So it suffices to show that $Z_t \in \spn\{Y_t, Y_{t-1}, \ldots\}$\\
        Define $U_t = Y_t - \mathcal{P}_{\spn\{Y_k, k \leq t-1 \}}T_t$\\
        Then by Prop 3.2.1:
        \begin{align*}
            Y_t = U_t + \alpha_1 U_{t-1} + \alpha_2 U_{t-2} + \ldots + \alpha_{q-s} U_{t - (q-s)}\\
            \implies f_Y(\lambda) = \frac{\sigma^2}{2\pi} |\alpha(e^{-i \lambda})|^2 = \frac{\sigma^2}{2\pi} |\theta^*(e^{-i \lambda})|^2
        \end{align*}
        Both $\alpha(z)$ and $\theta^*(z)$ are real polynomials of the same degree $q-s$ \\
        Factoring $\alpha(z) = \prod_{j=1}^{q-s} (1 - c_j^{-1} z)$ And $\theta^*(z) = \prod_{j=s+1}^{q} (1 - b_j^{-1} z)$\\
        We see that they must have the same roots ie $\{c_j\} = \{b_j\}$ as multisets\\
        And they are the same everywhere\\
        Thus $\alpha(z) = \theta^*(z)$\\
        Then using this we can show that $Z_t = U_t$ 
    \end{proof}
\end{proposition}
\subsection{10/30/2025 Lecture 17}
\begin{proposition}[4.4.1 cont] 
    Consider the ARMA(p,q) $\phi(B) X_t = \theta(B) Z_t$ where $\theta(z) \neq 0$ for $|z| < 1$\\
    Then $Z_t \in \spn\{X_t, X_{t-1}, \ldots\}$\
    \begin{proof}
        WLOG: assume $\theta(z) \neq 0$ for $|z| \neq 1$\\
        Define $Y_t = \theta(B) Z_t$ then $\phi(B) X_t = Y_t$\\
        $\implies \spn\{Y_t, k \leq t\} \subset \spn\{X_t, k \leq t\}$\\
        So it suffices to show that $Z_t \in \spn\{Y_t, Y_{t-1}, \ldots\}$\\
        Define $U_t = Y_t - \mathcal{P}_{\spn\{Y_k, k \leq t-1\}} Y_t$\\
        Then by Prop 3.2.1:
        \begin{align*}
            Y_t = U_t + \alpha_1 U_{t-1} + \alpha_2 U_{t-2} + \ldots + \alpha_q U_{t-q}\\
            \implies f_Y(\lambda) = \frac{\sigma^2}{2\pi} |\alpha(e^{-i \lambda})|^2 = \frac{\sigma^2}{2\pi} |\theta(e^{-i \lambda})|^2
        \end{align*}
        Both $\alpha(z)$ and $\theta(z)$ are real polynomials of the same degree q \\
        And they have the same roots ie $\{c_j\} = \{b_j\}$ as multisets\\
        And they are the same everywhere\\
        Thus $\alpha(z) = \theta(z)$\\
        And $\sigma^2 = \sigma_n^2$\\
        $\implies$ that $(U_t, Y_t, Y_{t-1}, \ldots, Y_{t-q})$ and $(Z_t, Y_t, Y_{t-1}, \ldots, Y_{t-q})$ have the same covarience matrix\\
        $\implies$ $\mathcal{P}_{\spn\{Y_k, k \leq t-1\}} U_t = \mathcal{P}_{\spn\{Y_k, k \leq t-1\}} Z_t$\\
        $\implies$ $U_t = \lim_{n \to \infty} \mathcal{P}_{\spn\{Y_t, Y_{t-1}, \ldots, Y_{t-n}\}} U_t = \lim_{n \to \infty} \mathcal{P}_{\spn\{Y_t, Y_{t-1}, \ldots, Y_{t-n}\}} Z_t$\\
        Bescuase $\sigma_n^2 = \sigma^2$ it holds that $Z_t = U_t$\\
        If you show that two projections are equal with the same variance then the two random variables are equal.
    \end{proof}
\end{proposition}
\begin{proposition}[4.4.3]
    Assume the ARMA(p,q) $\phi(B) X_t = \theta(B) Z_t$ where $\phi(z) \neq 0$ for $|z| < 1$\\
    If $\theta(z) = 0$ for some $|z| \leq 1$ then $Z_t \notin \spn\{X_t, X_{t-1}, \ldots\}$\\
\end{proposition}
\begin{definition}[Wold Decomposition]
    \textbf{Aside}: Crimea-Wold Device to prove the multivariate CLT\\
    \textbf{Setup}: $X_t, t \in \mathbb{Z}$ is a zero-mean stationary process. \\
    Define $\mathcal{M}_n = \spn\{X_t, t \leq n\}$, $\mathcal{M} = \overline{\bigcup_{n \in \mathbb{Z}} \mathcal{M}_n} = \spn\{X_t, t \in \mathbb{Z}\}$\\
    Define $\mathcal{M}_{-\infty} = \bigcap_{n \in \mathbb{Z}} \mathcal{M}_n$\\
    \fbox{Look into Durret and Billinsley}
\end{definition}
\begin{definition}[Mean Squared Error]
    $\sigma^2 = E[X_{n+1}^2 - \mathcal{P}_{\mathcal{M}_n} X_{n+1}]$ is the mean squared error of the BLP of $X_{n+1}$ based on $\mathcal{M}_n$\\
    AKA the one step ahead prediction error variance
\end{definition}
\begin{definition}[Determinisitic Process]
    A stationary process $\{X_t\}$ is deterministic if $ \sigma^2 = 0$ ie $X_{n+1} \in \mathcal{M}_n$ for all $n \in \mathbb{Z}$\\
    ie $X_{n+1}$ can be perfectly predicted based on the infinite past $\{X_n, X_{n-1}, \ldots\}$\\
    Note that for all Process $\sum_{j=1}^n A_j e^{-i \lambda_j t}$ is deterministic for $A_j$ uncorrelated mean zero random variables\\
\end{definition}
\begin{theorem}[3.7.1 Wold Decomposition Theorem]
    If $\sigma^2 > 0$  then $X_t$ can we expressed as $X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j} + V_t$ where\\
    \begin{enumerate}
        \item $\psi_0 = 1$, $\sum_{j=0}^{\infty} |\psi_j|^2 < \infty$
        \item $Z_t \sim WN(0, \sigma^2)$ 
        \item $Z_t \in \mathcal{M}_t, \forall t \in \mathbb{Z}$
        \item $E[Z_t V_s] = 0, \forall t, s \in \mathbb{Z}$
        \item $V_t \in \mathcal{M}_{-\infty}, \forall t \in \mathbb{Z}$
        \item $V_t$ is deterministic stationary process
    \end{enumerate}
    \begin{proof}
        Define $Z_t = X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t$ Thus the $Z_t \sim WN(0, \sigma^2)$ and $Z_t \in \mathcal{M}_t$ which gives us number 2 and 3\\
        Project $X_t$ onto $\spn\{Z_t, Z_{t-1}, \ldots, Z_{t-n}\}$
        \begin{align*}
            \mathcal{P}_{\spn\{Z_t, Z_{t-1}, \ldots, Z_{t-n}\}} X_t &=: \sum_{j=0}^{n} \psi_{j} Z_{t-j}
        \end{align*}
        Where $\psi_j = \frac{\innerprod{X_t}{Z_{t-j}}}{\innerprod{Z_{t-j}}{Z_{t-j}}}$\\
        In particular $\psi_0 = 1$ thus We get number 1\\
        Define $V_{t} = X_t - \lim_{n \to \infty} \mathcal{P}_{\spn\{Z_t, Z_{t-1}, \ldots, Z_{t-n}\}} X_t$, $V_t \in \mathcal{M}_t$\\
        Then $E[Z_s V_t] = 0$ for all $s, t \in \mathbb{Z}$ which gives us number 4\\
    \end{proof}
\end{theorem}
\subsection{11/04/2025 Lecture 18}
\begin{remark}[Wold Decomposition]
    Recall the Linear Past:
    \begin{align*}
        \mathcal{M}_n &= \spn\{X_t, t \leq n\} \\
        \mathcal{M} &= \overline{\bigcup_{n \in \mathbb{Z}} \mathcal{M}_n} = \spn\{X_t, t \in \mathbb{Z}\}\\
        \mathcal{M}_{-\infty} &= \bigcap_{n \in \mathbb{Z}} \mathcal{M}_n
    \end{align*}
    $\sigma^2 = E[X_{n+1} - \mathcal{P}_{\mathcal{M}_n} X_{n+1}]$ is the one step ahead prediction error variance\\
\end{remark}
\begin{definition}[Determinisitic]
    A stationary process $\{X_t\}$ is deterministic if $ \sigma^2 = 0$ 
\end{definition}
\begin{theorem}[5.7.1 Wold Decomposition Theorem]
    If $\sigma^2 > 0$  then $X_t$ can we expressed as $X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j} + V_t$ where\\
    \begin{enumerate}
        \item $\psi_0 = 1$, $\sum_{j=0}^{\infty} |\psi_j|^2 < \infty$
        \item $Z_t \sim WN(0, \sigma^2)$ 
        \item $Z_t \in \mathcal{M}_t, \forall t \in \mathbb{Z}$
        \item $E[Z_t V_s] = 0, \forall t, s \in \mathbb{Z}$
        \item $V_t \in \mathcal{M}_{-\infty}, \forall t \in \mathbb{Z}$
        \item $V_t$ is deterministic stationary process
    \end{enumerate}
    The sequences $\{\psi_j\}$, $\{Z_t\}$ and $\{V_t\}$ are uniquely determined by the assumptions above.\\
    \begin{proof}
        $Z_t = X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t$\\
        $\psi_j = \frac{\innerprod{X_t}{Z_{t-j}}}{\innerprod{Z_{t-j}}{Z_{t-j}}}$
        $V_t = X_t - \sum_{j=0}^{\infty} \psi_j Z_{t-j}$\\
        $\mathcal{M}_t = \mathcal{M}_{t-1} \oplus \spn\{Z_t\}, V_t \perp Z_t \implies V_t \in \mathcal{M}_{t-1}$\\
        $\mathcal{M}_t = \mathcal{M}_{t-2} \oplus \spn\{Z_{t-1}, Z_t\} , V_t \perp Z_{t-1}, Z_t \implies V_t \in \mathcal{M}_{t-2}$\\
        Continuing this way we get $V_t \in \mathcal{M}_{t-k}$ for all $k \geq 0$ thus $V_t \in \mathcal{M}_{-\infty}$\\
        This shows properties 5.\\
        Let $\mathcal{M}_t^{V} = \spn\{V_t, V_{t-1}, \ldots\}$\\
        Need to show that $V_t \in \mathcal{M}_{t-1}^{V}$ 
        and thus will show that $V_t \in \mathcal{M}_{-\infty}^{V}$\\
        it suffices to show that $\mathcal{M}_{-\infty}^{V} = \mathcal{M}_{-\infty}$\\
        We know $V_T \in \mathcal{M}_t \implies \mathcal{M}_{t}^{V} \subset \mathcal{M}_t \implies \mathcal{M}_{-\infty}^{V} \subset \mathcal{M}_{-\infty}$\\
        Now we show the reverse inclusion.\\
        $\mathcal{M}_t = \spn\{Z_k, V_k, k \leq t\} = \spn\{Z_k, k \leq t\} \oplus \mathcal{M}_t^{V}$\\
        $\mathcal{M}_{t-k} \perp \spn\{Z_{t} \dots Z_{t-k}\}$\\
        $\implies \mathcal{M}_{-\infty} \perp \spn\{Z_t, Z_{t-1}, \ldots\} \implies \mathcal{M}_{-\infty} \subset \mathcal{M}_{t}^{V}$\\
        Thus $\mathcal{M}_{-\infty} \subset \mathcal{M}_{-\infty}^{V}$\\
    \end{proof}
    \begin{proof}[Proof of Uniqueness]
        If $X_t = \sum_{j=0}^{\infty} \eta_j W_{t-j} + G_t$ with the same properties as above.\\
        $(5)$ implies that $E[G_t] \in \mathcal{M}_{-\infty}$\\
        $(3)$ implies that $W_{t-1}, W_{t-2}, \ldots \in \mathcal{M}_{t-1}$\\
        \begin{align*}
            X_t = W_t + \sum_{j=1}^{\infty} \eta_j W_{t-j} + G_t \in \mathcal{M}_{t-1}
        \end{align*}
        And $W_t \perp X_{t-k}$ for any $k \geq 1$\\
        Thus $W_t = X_t - \mathcal{P}_{\mathcal{M}_{t-1}} X_t = Z_t$\\
    \end{proof}
\end{theorem}
\begin{theorem}[Kolmogorov or Doob]
    Let $\{Y_{1t}\}$ and $\{Y_{2t}\}$ be two zero-mean mutually orthogonal stationary processes and let $X_t = Y_{1t} + Y_{2t}$ Suppose $F_1$ and $F_2$ are the spectral distribution functions of $\{Y_{1t}\}$ and $\{Y_{2t}\}$ respectively. \\
    Then $\{ Y_{1t}, Y_{2t}, t \in \mathbb{Z}\} \subset \mathcal{M}^X := \spn\{X_k, k\in \mathbb{Z}\}$ \\
    IFF\\
    $F_1$ and $F_2$ are mutually singular ie $F_1$ corresponds to a measure that is on the space $m_1$ and $F_2$ corresponds to a measure that is on the space $m_2$. If there exists a measureable set $A \subset (-\pi, \pi]$ such that, $m_1(A^c) = 0$ and $m_2(A) = 0$ then $F_1$ and $F_2$ are mutually singular.\\
\end{theorem}
\begin{theorem}[Rudin]
    IF $\{c_n, n \leq 0\}$ are s.t. $0 < \sum_{n=-\infty}^{0} |c_n|^2 < \infty$ then $ \sum_{n=-\infty}^{0} c_n e^{i n \lambda} \in L^2(-\pi, \pi] : = \{g : (-\pi, \pi] \to \mathbb{C} | \int_{-\pi}^{\pi} |g(\lambda)|^2 d\lambda < \infty\}$\\
    and $\int_{-\pi}^{\pi} log|c e^{i \theta}|d\theta > -\infty$
\end{theorem}
\begin{remark}[Back to Wold]
    $\sigma^2 > 0$ let $\psi(e^{-i \lambda}) = \sum_{n=0}^{\infty} \psi_j e^{-i n \lambda}$\\
    By Theorem Rudin $\int_{-\pi}^{\pi} log|\psi(e^{-i \lambda})| d\lambda > -\infty \implies \psi(e^{-i \lambda})$ is non-zero almost everywhere on $(-\pi, \pi]$.
    Let $U_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j}$ then $U_t$ and $V_t$ (deterministic part) are orthogonal.\\
    Let $f_U(\lambda)$ and $f_V(\lambda)$ be the spectral density functions of $U_t$ and $V_t$ respectively.\\
    Then $f_X(\lambda) = f_U(\lambda) + f_V(\lambda)$ and $f_U(\lambda)$ and $f_V(\lambda)$ are mutually singular.\\
    Can show that $\frac{\sigma^2}{2\pi}|\psi(e^{-i \lambda})|^2 $ is the spectral density function of $U_t$\\
    $\implies F_u$ is absolutely continuous almost everywhere with non-zero, non-negative density function.\\
    $F_v$ is singular with respect to $F_u$ and singular to the Lebesgue measure.\\
    IE $\exists$ a set $A$ with Lebesgue measure $0$ such that $F_v(A^c) = 0$\\
\end{remark}
\subsection{11/06/2025 Lecture 19}
\begin{theorem}[5.6.1 Wold Decomposition]
    If $\sigma^2 > 0$  then $X_t$ can we expressed as $X_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j} + V_t$ where\\
    \begin{enumerate}
        \item $\psi_0 = 1$, $\sum_{j=0}^{\infty} |\psi_j|^2 < \infty$
        \item $Z_t \sim WN(0, \sigma^2)$ 
        \item $Z_t \in \mathcal{M}_t, \forall t \in \mathbb{Z}$
        \item $E[Z_t V_s] = 0, \forall t, s \in \mathbb{Z}$
        \item $V_t \in \mathcal{M}_{-\infty}, \forall t \in \mathbb{Z}$
        \item $V_t$ is deterministic stationary process
    \end{enumerate}
    The sequences $\{\psi_j\}$, $\{Z_t\}$ and $\{V_t\}$ are uniquely determined by the assumptions above.\\
\end{theorem}
\begin{remark}
    \begin{enumerate}
        \item The spectral density function of $U_t = \sum_{j=0}^{\infty} \psi_j Z_{t-j}$ is absolutely continuous wrt the Lebesgue measure ie $F_U(\lambda) = \int_{-\pi}^{\lambda} f_U(\nu) d\nu$ and we know it is given by $f_U(\lambda) = \frac{\sigma^2}{2\pi} |\psi(e^{-i \lambda})|^2$ where $\psi(e^{-i \lambda}) = \sum_{j=0}^{\infty} \psi_j e^{-i j \lambda}$. Furthermore $\psi(e^{-i \lambda}) \neq 0$ almost everywhere on $(-\pi, \pi]$.
        \item $F_U$ and $F_V$ are mutually singular ie $\exists$ a set $A$ with Lebesgue measure $0$ such that $F_V(A^c) = 0$ and $F_U(A) = 0$
        \item $F_X = F_U + F_V$ is the Lebesgue decomposition of $F_X$ wrt the Lebesgue measure.
    \end{enumerate}
    Let $F_X$ be the spectral distribution function of $X_t$. Let $f_X(\theta)$ be its derivative, then we can write $F_X(\lambda) = \int_{-\pi}^{\lambda} f_X(\theta) d\theta + F_s(\lambda)  F_c(\lambda) + F_s(\lambda)$ where $F_s$ is also a distribution functions
    \begin{enumerate}
        \item $f_x(\theta) = 0$ on a set of positive measure
        \item $f_x(\theta) > 0$ almost everywhere $\int_{-\pi}^{\pi} log f_X(\theta) d\theta = -\infty$
        \item $f_x(\theta) > 0$ almost everywhere $\int_{-\pi}^{\pi} log f_X(\theta) d\theta > -\infty$
    \end{enumerate}
\end{remark}
\begin{theorem}[Kolmogorov Formula]
    $\sigma^2 > 0$ iff $\int_{-\pi}^{\pi} log f_X(\lambda) d\lambda > -\infty$\\
    and $\sigma^2 = 2\pi \exp\left(\frac{1}{2\pi} \int_{-\pi}^{\pi} log f_X(\lambda) d\lambda \right)$
\end{theorem}
\begin{example}
    \begin{align*}
        (1-0.5B) X_t = (1+0.3B) Z_t \quad Z_t \sim WN(0,1)\\
    \end{align*}
    $f_x(\theta) = \frac{\sigma^2}{2\pi} \cdot \frac{|1+0.3 e^{-i \theta}|^2}{|1-0.5 e^{-i \theta}|^2}$
    \begin{align*}
        \log(f_x(\theta)) &= \log\left(\frac{\sigma^2}{2\pi}\right) + 2 \log|1+0.3 e^{-i \theta}| - 2 \log|1-0.5 e^{-i \theta}|\\
        &\implies \int_{-\pi}^{\pi} \log(f_x(\theta)) d\theta = 2\pi \log\left(\frac{\sigma^2}{2\pi}\right) 
    \end{align*} 
\end{example}
\begin{remark}
    Why? are we doing this.\\
    $X_t = \sum_{j=1}^n A_j e^{i \lambda_j t}$ where $A_j$ are uncorrelated mean zero random variables.\\
\end{remark}
\begin{remark}[Infrence For ARMA Models]
    Not only we do we have coefficients $\phi$ and $\theta$ to estimate but we also have to determine the order of the model $p$ and $q$.\\
    Suppose we get data for ARMA(p,q): $\phi(B) (X_t - \mu) = \theta(B) Z_t$ where $Z_t \sim WN(0, \sigma^2)$\\
    Assume $p, q$ are known.\\
    We estimate mean $\mu$, by $\hat \mu = \bar X_n = \frac{1}{n} \sum_{t=1}^{n} X_t$\\
    Suppose $\{X_1 \ldots X_n\}$ is a realization of a sp. \\
    \begin{align*}
        \hat{\mu} &= \bar{X}_n = \frac{1}{n} \sum_{t=1}^{n} X_t\\
        \bar{X}_n &\xrightarrow{a.s.} E[X_0 | \mathcal{M}_{-\infty}] \text{ as } n \to \infty
    \end{align*}
    When $\bar{X} \xrightarrow{a.s} \mu$ then we call the process ergodic in the mean.\\
    \begin{enumerate}
        \item IID sequence with finite mean is ergodic
        \item Suppose $Z_t$ iid 
        \item $X_t := g(Z_t, Z_{t-1}, \ldots)$ and $g$ is measurable and $E[|X_t|] < \infty$ then $X_t$ is ergodic.
    \end{enumerate}
\end{remark}


\end{document}